<!DOCTYPE html>

<html lang="en-us">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">
  

  <title> - Data reporting @ Cronkite</title>
  <link rel="stylesheet" href="http://localhost:4000/cronkite-docs/assets/css/just-the-docs.css">
  
  <script type="text/javascript" src="http://localhost:4000/cronkite-docs/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>

  <div class="page-wrap">
    <div class="side-bar">
      <a href="http://localhost:4000/cronkite-docs" class="site-title fs-6 lh-tight">Data reporting @ Cronkite</a>
      <span class="fs-3"><button class="js-main-nav-trigger navigation-list-toggle btn btn-outline" type="button" data-text-toggle="Hide">Menu</button></span>
      <div class="navigation main-nav js-main-nav">
        <nav role="navigation" aria-label="Main navigation">
  <ul class="navigation-list">
    
    
      
    
      
    
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/cronkite-docs/" class="navigation-list-link">Home</a>
            
          </li>
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/cronkite-docs/general" class="navigation-list-link">General resources</a>
            
              
              <ul class="navigation-list-child-list ">
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/general/newsroom-math.html" class="navigation-list-link">Newsroom math</a>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/general/file-formats.html" class="navigation-list-link">File formats</a>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/general/data-diary.html" class="navigation-list-link">Data diaries</a>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/general/sourcing.html" class="navigation-list-link">Data sources and sourcing</a>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/general/story-list.html" class="navigation-list-link">Story list</a>
                      
                    </li>
                  
                
                  
                
              </ul>
            
          </li>
        
      
    
      
        
      
    
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/cronkite-docs/excel" class="navigation-list-link">Excel</a>
            
              
              <ul class="navigation-list-child-list ">
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/excel/xlguides" class="navigation-list-link">Excel guides</a>
                      
                        
                        <ul class="navigation-list-child-list">
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/xlguides/xl-refresher.html" class="navigation-list-link">Excel refresher</a>
                              </li>
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/xlguides/xl-tidydata.html" class="navigation-list-link">Data types / tidy data</a>
                              </li>
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/xlguides/xl-filtersort.html" class="navigation-list-link">Filter and sort</a>
                              </li>
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/xlguides/xl-formulas.html" class="navigation-list-link">Excel formulas</a>
                              </li>
                            
                          
                            
                          
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/xlguides/xl-pivot.html" class="navigation-list-link">Grouping with pivot tables</a>
                              </li>
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                        </ul>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/excel/xlpractice" class="navigation-list-link">Excel practice</a>
                      
                        
                        <ul class="navigation-list-child-list">
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/practice/excel-azpop-exercise.html" class="navigation-list-link">Arizona population exercise</a>
                              </li>
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/practice/excel-police-uof.html" class="navigation-list-link">Use of force database</a>
                              </li>
                            
                          
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/practice/xl-kpop.html" class="navigation-list-link">K-pop</a>
                              </li>
                            
                          
                            
                          
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/practice/excel-rates.html" class="navigation-list-link">Rates and ratios with FBI data</a>
                              </li>
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/practice/excel-baseball.html" class="navigation-list-link">Arizona Diamondbacks scores</a>
                              </li>
                            
                          
                            
                              <li class="navigation-list-item ">
                                <a href="http://localhost:4000/cronkite-docs/excel/practice/excel-pivot-pools.html" class="navigation-list-link">Swimming pool inspections</a>
                              </li>
                            
                          
                            
                          
                            
                          
                            
                          
                        </ul>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
              </ul>
            
          </li>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/cronkite-docs/r-stats" class="navigation-list-link">R for journalism</a>
            
              
              <ul class="navigation-list-child-list ">
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
              </ul>
            
          </li>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/cronkite-docs/special" class="navigation-list-link">Special</a>
            
              
              <ul class="navigation-list-child-list ">
                
                  
                
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/special/cleanup_medicare.html" class="navigation-list-link">Open Refine 2</a>
                      
                    </li>
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/special/cleanup_pdf.html" class="navigation-list-link">PDF tools</a>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
              </ul>
            
          </li>
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/cronkite-docs/records" class="navigation-list-link">Public records</a>
            
              
              <ul class="navigation-list-child-list ">
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/foia/public-records-outside-in.html" class="navigation-list-link">Public records strategey</a>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
              </ul>
            
          </li>
        
      
    
      
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/cronkite-docs/workshops" class="navigation-list-link">Workshops</a>
            
              
              <ul class="navigation-list-child-list ">
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/workshops/cronkite-mayo.html" class="navigation-list-link">Cronkite-Mayo</a>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/workshops/bootcamp.html" class="navigation-list-link">Cronkite bootcamp</a>
                      
                    </li>
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/workshops/phd-sources.html" class="navigation-list-link">Links to source data for doctoral students</a>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                    <li class="navigation-list-item ">
                      
                      <a href="http://localhost:4000/cronkite-docs/workshops/fundamental1day.html" class="navigation-list-link">1-day CAR workshop</a>
                      
                    </li>
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
                  
                
              </ul>
            
          </li>
        
      
    
  </ul>
</nav>

      </div>
      <footer role="contentinfo" class="site-footer">
        <p class="text-small text-grey-dk-000 mb-0">This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</p>
      </footer>
    </div>
    <div class="main-content-wrap">
      <div class="page-header">
        <div class="main-content">
          
          
            <ul class="list-style-none text-small mt-md-1 mb-md-1 pb-4 pb-md-0 js-aux-nav aux-nav">
              
                <li class="d-inline-block my-0"><a href="https://cronkite.asu.edu/about/faculty-and-leadership/faculty/cohenbio">Sarah Cohen / Knight Chair / Cronkite School</a></li>
              
            </ul>
          
        </div>
      </div>
      <div class="main-content js-main-content" tabindex="0">
        
          
        
        <div id="main-content" class="page-content" role="main">
          {
  "0": {
    "id": "0",
    "title": "",
    "content": "##Lecture NotesSept. 24, 2014 Tip sheet is in this folder. Section 1: Review of last week’s assignment Remember deadlines. I will cut off the ability to post after deadline in the future. It’ll be as if you didn’t do it. Remember grammar / usage. This isn’t a writing class, but you should get used to writing at a professional level in all of your work. (I’m one to talk…) Question: I asked everyone to keep an eye out for clues of public records – remember the sticker on the fire extinguisher? My story about the “federal law” requiring handicapped and elderly reserved seats? What did you see this week? I read all of the homework – most of it good. The Seattle and Post ones were hard, since there was no IRE form. Some read it more carefully than others. I’ll release individual comments to you today or tomorrow. (Wed-Thur) Housekeeping: I updated the syllabus. Reading added for next week: “In the Line of Fire,” by CUNY graduate students (IRE finalist 2013), on innocent bystanders. “Inside the Hidden World of Thefts…”, Washington Post on non-profits. “Behind Big Political Gifts, a Mysterious Donor,” New York Times “Cuomo’s Office Hobbled Ethics Inquiries by Moreland Commission”, New York Times I will change the final project – you guys are way more prepared for this course than my previous group. I think you can handle something just a little more challenging. Discussion of stories: Seattle Times’ coverage of officers shot Hazard in the Heartland Washington Post coverage of the Virginia Tech shootings Section 2: Review types of sources and documents The homework suggested that we’re not entirely clear what types of sources are what. Review type and what you can do with them. Human sources: Spokesmen, witnesses, actors in the process.What biases and distractions do they bring to the story? What is the motivation? Secondary sources: News reports in particular, references to original documents or interviews through others. Getting public records from sources other than the government or source that created them is considered a secondary source (eg, Nexis search of public records, or court document provided on another website). Primary sources: Official documents, notes and records created at the time of an event for a business reason. Examples: court records (to administer justice); property records (to buy and sell property); birth records (to record your arrival in the world, ascertain citizenship). The special case of social media: somewhere in between, because it might be a primary source, but it also isn’t official and can be made up. We have to credit / source secondary sources. We have to confirm social media and human sources. We can normally publish what we find in official primary sources, but you have to be very careful to make sure you have confirmed that it refers to the right person. Section 3: Backgrounding on deadline Reasons to have to background on deadline: Find people to interview: witnesses to an accident; neighbors; relatives. Write about a previously unknown person who is thrust into the news. Write about an event that intersects with government agencies (virtually all of them do): plane crashes; bridge failures; mass shootings; oil spills, etc. Advanced Nexis and Factiva searching. Strategies: Use Doug Haddix’s librarian checklist from his presentation. Answer questions: What do they say about themselves? (People, organization, agency, etc.) What do others say about them? What is left out? What’s confirmed or untrue? Outside-in reporting: As quickly as possible, identify all names used (middle, maiden, etc.) and DOB. You need this to know whether you are likely talking about the same John Smith. Nexis / Factiva search Social media accounts Nexis / Google News / Google Scholar Advanced Google search Other search engines Now move to public records: We’ll concentrate on backgrounding people today. I want to walk through a couple of sources that are a little difficult to navigate if you never have before: Property records - using NYC finance dept. Court records - using NY State WebCivil. Record what you’ve done and what you haven’t – what can you publish and what can’t you? Spreadsheet with source, link, etc. In the best of all worlds, you would print the copy of the web page to a PDF file, keep in on hand. Web sites change, especially social media sites that are associated with suddenly famous people. Record what you can publish, what needs confirmation, what is off the record or against your news organization’s rules (we don’t re-publish information from other news organizations without permision). Don’t get side-tracked. You need to stay focused on what you are doing TODAY. Leave the deep research for another day. Section 4: In class We’ll do an exercise of backgrounding in an hour. We’ll see where we are at the end of class – I want everyone to feel like they got some experience without getting overwhelmed. I may ask them to spend 1 more hour (not more than that) and then turn it in.",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/foia/backgrounding-class-readme.html",
    "relUrl": "/assets/docs/foia/backgrounding-class-readme.html"
  },
  "1": {
    "id": "1",
    "title": "Cronkite bootcamp",
    "content": "FOIA The Art of Access, by David Cullier, is a great book for introducing you to techniques and approaches – ways to get to “yes”. It will be required in the data reporting class if you take it. Cronkite School’s FOIA resource pages Numbers in the newsroom Slides from class Handout with common formulas for change, percents, etc. Excel refresher We’ll work from a few tutorials: Quick Excel refresher. Look here for keyboard shortcuts and short clips reminding you how to do basic navigation. Get used to the concept of tidy data) Excel files we’ll work with: The budget from Phoenix An example of a homemade database",
    "url": "http://localhost:4000/cronkite-docs/workshops/bootcamp.html",
    "relUrl": "/workshops/bootcamp.html"
  },
  "2": {
    "id": "2",
    "title": "Bulletproofing",
    "content": "Bulletproofing the data project Here are some handouts on avoiding data errors in stories (all from IRE, so you’ll need to sign in): “Editing the data-driven story,” by Maud Beelman now at the Associated Press: https://www.ire.org/resource-center/tipsheets/4045&gt; DANGER! Look out for Dirty Data, by Jaimi Dowdell now at Reuters http://ire.org/resource-center/tipsheets/3999 “A Guide to Bulletproofing Your Data,” by Jennifer LaFleur, now at the Investigative Reporting Workshop at American University: http://ire.org/resource-center/tipsheets/3848 I wrote this email in May 2014 to Criag Silverman, who at the time wrote a column for Poynter called “Regret the Error”. The topic was how we keep from making mistakes in projects that rely on a lot of records and data. I can’t find the piece that it was used in, but here’s the raw email text: Report out the underlying records Data definitions &amp; codes Integrity checks Find a rabbi or a sherpa The data diary Report out cases Vetting results Editing and fact-checking Sarah Cohen The New York Times May 1, 2014 Report out the underlying records All data comes from some kind of individual record – it’s a survey response, a traffic ticket, or a payment. So I try to track the statistics back to the underlying records and determine how they were collected and how they make their way into some kind of data system. Most still come from some kind of form, which – even if it is transmitted electronically and in bulk – still has a detailed description or a manual somewhere. Those forms tell you a lot: How much discretion do people have? What do the instructions say? Is it confusing? Then I talk to people who fill them out and people who process them once they’re filled out. I also go through hearings, audits and other criticisms of the system. Say you’re looking at crime stats: what do the underlying incident reports look like? What decisions are cops making, and what are they rewarded or punished for doing that might push close calls one way or another? What gets counted or ignored? What problems have they had in the past in responding to incidents or writing them up? I also look for items that never get into the system at all. In this case, it might be crimes that got downgraded, or people who don’t ever call the police (maybe like domestic violence victims who might lose their lease for repeated complaints.) This is a reporting job, not a data analysis job – one that too many people skip. I just did a story on deportations, and ICE only maintains the “most serious” criminal conviction – It took about 3 weeks to nail down what that meant, who assigns it and how it matters to the agents on the ground. Data definitions &amp; codes I try to go through each field in the database and be sure I know what each one actually means, not just what it says, even if it seems irrelevant to what I’m doing. The names and the descriptions of codes are usually some kind of bureaucratic or computer-ese shorthand, and they often aren’t what they seem. There’s a famous example of this: old versions of the federal contracts database had a field called “obligation type”. If it was “A”, it meant the amount listed was a payment by the government to the contractor. If it was “B”, it meant that there should be a minus sign next to the amount – it was money the government got back from the contractor. Yikes. I try out a few sentences using the data and see what might be wrong with them. In the immigration example, a single person could be deported many times, but the agency wouldn’t give us anything that would let us track an individual. It meant we could never characterize the data as “people”, only “cases.” Integrity checks I try to find some kind of benchmark I should be able to hit pretty closely (or exactly). In the immigration example, the agency had published a few key statistics from the same data, and once I got the definitions down, I could match them almost exactly. There was a good explanation about why they would have changed since they published the initial figures, and they were only off by about 100 out of 300,000. Most data reporters start laughing when they see a spreadsheet of 65,536 rows - it’s the limit of older versions of Excel and usually indicates that dataset isn’t complete. (Early in the Wikileaks story, the reporters were stumped because they were missing the later months. The problem was they’d imported it into a spreadsheet with those limits. They realized it quickly, but they might not have if they weren’t diligent.) I also try to do simple frequencies on any field I care about to see how often they’re filled out and whether they seem to have any oddities. A lot of times you’ll find out no one has ever used the code you think is so interesting, or even that they have redacted the cases with a code you might care about. The Washington Post won a Pulitzer in 1999 for a series of stories about police shootings that originated from a missing code – the FBI had been removing justifiable homicide by police (maybe “81”) from its Supplemental Homicide Reports for years, and Jo Craven McGinty fought to get those missing records. The story, of course, was far more than the data, but the data sparked them to look at the problem of police shootings in DC. I also look for things like missing months, zip codes that don’t exist, impossible combinations like 10-year-olds with court records, or codes that abruptly start or stop. When I find these problems, I have to report them out. Find a rabbi or a sherpa Others know this data better than I do and I try to convince them to guide me and look over my shoulder. Ideally, there are 3-4 people who you can trust to vet your work along the way. This usually requires convincing them you’re worth their time by doing a lot of the steps above and by reading anything they’ve written on the subject. It’s also useful to find any academic literature on your topic and contact any researchers who have touched on it. They often know of projects they would have wanted to do, but couldn’t get funded. The data diary On longer stories, I keep a diary of the data work three ways, but different people do it differently. The goal is to make the work reproduce-able and to make sure I’ve got reasons for every decision I had to make along the way. Use computer programs to do as much as possible, heavily commented and versioned. I haven’t gotten in the habit of saving my versions on Github, but I should – it’s a great way to be able to backtrack. It’s much more difficult to log every mouse click than to to comment computer code to say what problem I was trying to solve or question I was trying to ask, and what problems I ran into doing it other ways. It’s really easy to fix and re-run a computer program than it is to re-do mouse clicks when it turns out there was a mistake early on. Keep a log of my interviews on the data. Invariably, you get conflicting answers or never get satisfactory answers to some of the questions. The problem is, usually no one has used the data the way you’re using it, so you have to ask them why what you are doing could be wrong – what traps are in there waiting for you? I do agree to these interviews on background. I know that others don’t, but at this point I’d rather know what I might be doing wrong, and I don’t really have any firm results yet. Keep another log decisions I made along the way. Again on the immigration story, I had to decide what a “minor” vs. “serious” crime was, and what to do about drug convictions when it didn’t distinguish between sale or possession. I try to make every close call err on the side of the least newsworthy answer to avoid hyping results. In this case, drug offenses were counted as “serious” all the time, because the agency claimed it was focusing on “serious” criminals and we didn’t want to undercount them when we said that most deportees weren’t. Report out cases I describe the process as moving from lab to field and back to the lab, reporting out some cases that seem to reflect a bigger pattern. Those often turn into the anecdotes in the story. In some cases, you can look yourself up or someone you know. In others, you can look up cases that have been in the news. In others, you can go to a place and talk to some actors. When I worked on farm subsidies, we called lots and lots of farmers and asked them to go over what we thought we’d found. We also try to find ex-employees of the agency or a company to go over the records with us. But usually, there was one case that sparked our interest in the records in the first place, so we can look up that case and find others like it. Last year, I worked on a story on police who committed domestic violence. One set of records I used were disciplinary records for one state, which showed a huge drop in 2012. I kept asking the agency why, and what was missing, but the expert on the data said it was complete. Later, I filed a public records request for a recent case that was in the news. That’s when I learned that the entire case is secret until it goes to a hearing, which often happens 6-8 months after the complaint. Those were the missing records, but I would have never known if I hadn’t been asking about cases. (It also helps with the public records aspect of the issue – by giving us enough information to report out cases, the agency can be sure that the data isn’t misconstrued, and it reduces the agency’s workload in explaining things.) Vetting results I want to give the agency or the subject of the time to digest what we’ve found and seek out as many experts and stakeholders as I can to vet the results. There is always a risk that there is some kind of mistake in logic or processing or some other misunderstanding. I want to hear the reasons I might be wrong, and about anything that surprises them. In most of these projects, no one has ever done what I’ve done, so they can’t fact check for me. One thing that has made this really important step a lot harder is that government agencies now restrict who we can talk with so much that we can’t get a real experts to answer questions at the front end. That means we have to try to engage them at least at the back end. It’s far from ideal, but even worse when the agency just won’t let us talk with anyone but the PIO or the agency head, who may not know enough about the details to give us real feedback. We used to be able to engage with the people who were in charge of the data collection, even if it was just on background. Editing and fact-checking I write the sections of the story that I’m responsible for when I work on a team, and go through drafts regularly to find places that the records could strengthen a point. This sometimes involves reworking the data so that it’s more precisely matching the sentences we want to write – for instance, “10 years” not “since 2002”. (In other words, cut off a year of the data.) I’m in the editing sessions or getting copies from the editor on a regular basis, as well as coordinating any data with graphics as we go along. One problem is that there are often old versions of results or spreadsheets floating around among editors, reporters and graphics folks, and they sometimes find their way into drafts over and over, after we’ve made corrections based on our vetting. For me, fact-checking involves circling every fact – whether or not it’s a number – and going back into the data and re-doing the analysis that led to that particular fact. Then I’ll pull that out into a fact-check file somewhere. At some point during the writing / editing phase, I try to retrace the entire process from start to finish. For instance, in the immigration story, the most time-consuming thing was to combine 11 spreadsheets into one database containing 3 million records. I didn’t re-do that step because I had been able to match my benchmarks after that step. But I do re-do any key tables or queries, check the code that created them, and make sure that they’ve incorporated any of the comments I got along the way. I’m also checking the graphics regularly to make sure they match what the story says and resolve any differences.",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/bulletproof.html",
    "relUrl": "/assets/docs/bulletproof.html"
  },
  "3": {
    "id": "3",
    "title": "ire-toronto",
    "content": "ire-toronto Scraping may be the most valuable new skill you’ll learn this year. There are several tools out there to help you, with more coming out all the time. But at some point, if you do this enough, you’ll find that learning to program a little is a lot easier than trying to wrangle a tool into something it didn’t anticipate. What’s scraping, and what can you do with it? With the simplest of tools: Download all of the documents linked off of a page. Simple scrape of a table created on one page, like inmates at the Boone County jail or salaries of officials in Ontario’s legislature Simple scrape of a more complex page, like a page of articles from the IRE website, Extra Extra, or Craigslist listings for housing in Toronto With some common tools: Extract everything from a set of paginated results. Dig into detail pages when you have to click on each link on a site to get more. Simulate simple searches, like going through each possible date on the White House public schedule. What’s going to be hard no matter what: Security systems that require Javascript and checking of session cookies. Search pages that require parameters you can’t know in advance Popups Most of these can be finessed with enough programming, but your simple tools probably won’t do it. ##Tools #####Free and easy DownloadThemAll! for Firefox, Download Master for Chrome. These will let you capture all of the files linked off a page, filtering for just the ones you want. They are great when an agency has put a whole bunch of PDFs online and you want them all in one folder so you can search them. Some people just use Google Docs for scraping simple pages, but I find them limiting and the formulas just don’t work that well. Chrome Scraper extension, for parsing almost anything you can see on a page. Very powerful. #####Not too expensive and more powerful Outwit Hub](http://www.outwit.com/), US$60 - standalone software or Firefox extension that automates going through multipage results and digging into detail pages. Helium Scraper, US$99 for Windows only. More powerful for very complicated pages, but it helps to know a little Javascript if you need to walk through forms or guess addresses. Each of these have their own odd language, so there’s a considerable learning curve. If you’re spending a lot of time learning the tool, consider putting your effort instead into learning a programming language like Ruby or Python for scraping.",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/chrome_scraper.html",
    "relUrl": "/assets/docs/chrome_scraper.html"
  },
  "4": {
    "id": "4",
    "title": "Open Refine 2",
    "content": "Data cleanup: XL, Google / Open Refine, intro to regex Example: Managed Long Term Care Enrollment reports. Came from this site: http://www.health.ny.gov/health_care/managed_care/reports/enrollment/monthly/ We had a very simple question, one that wasn’t worth very much work. Which had the biggest change? ##Step 1: Download all of the Excel spreadsheets and skip the pdfs DownloadThemAll! as a Mozilla add-on is easy. Could easily be programmed in Python, Ruby or any other. If you’re good with command line tools: wget. ##Step 2: Find the commonality among the sheets you want. Look at some of the examples, go through a few years just to see if the format changed dramatically at some point. In some, the tab is called “Managed Long Term Care”, in others it’s called “Long Term Care”. We only care about the PACE programs, and they appear to always be in a sheet called one of those two things. ##Step 3: Combine them all into one sheet, stacked on top of each other. I determined that this was easier than trying to anticipate the exact form in a program. But how? Use Python or Ruby packages to copy and paste them into one sheet. _#fail__ . There were errors in the files that prevented it - none of the standard packages would read these Excel files, I think because there were bad references in them. Write an Excel VBA program to do the same thing. That might have been easier, but I’m rusty at VBA. I’m sure I could have done it, but why? Seek out a tool to do the job. A Google search on “combine the same worksheet from multiple workbooks” yielded just the tool I needed: KuTools. In the end I had a spreadsheet like this (I hid a bunch of rows in the middle): ##Step 4: Make sure all of the sheets got copied I actually had to go back and manually copy a few. What do you think an easy way to see whether I have every month might be? How would you filter to see what months were captured? ##Step 5: Now go to Open Refine for the dirty work This could have been cleaned up in Excel using much of the same method we’re going to use in OpenRefine, but there are a couple of advantages. OpenRefine saves all of your steps. If you have to troubleshoot the steps, you can always look at the diary. OpenRefine has quick commands to do common things that Excel doesn’t. It’s possible in Excel, but it’s a little more clunky. Regular expressions Once you’re done, it’s easy to standardize the company names. Watch out, though - Make sure that if you get it right, you don’t go back too far. You could write over the right ste ##Step 6: Work out a strategy to refine it. You need to turn it into a database, not a report. This means keeping some of the totals, but getting rid of much of the summary; keeping a date next to each entry and distinguishing the PACE projects from the partial capitation. Key commands: value.match(regex) . (Note: OpenRefine requires wildcards before and after the phrase if they aren’t the beginnings and ends.) reversing the order permanently to fill down the totals, then reversing it back. Copying the original name and clustering to standardize. working out which rows to keep and which rows to trash. These can be regex filters or you can use the list. Once it’s out of open refine, you could bring it into something like Tableau Public to make this:",
    "url": "http://localhost:4000/cronkite-docs/special/cleanup_medicare.html",
    "relUrl": "/special/cleanup_medicare.html"
  },
  "5": {
    "id": "5",
    "title": "PDF tools",
    "content": "#Dealing with PDFs PDF’s are the bane of a data journalist’s existence. They come in all kinds of unfriendly formats. Is it an image or is it text? It doesn’t matter if you just want to search a pdf or turn it into a data set – if it starts as an image, you’ve got a big hurdle to jump called Optical Character Recognition, or OCR. Learn to check for image vs. text as soon as you get a document. THe easiest way is to try to select some text in your pdf reader. If you can’t, it’s probably a picture of text. If you can, then try to select some more. If you can select most of it, it’s probably text. Government agencies will give you both kinds. The easiest to use are those that were created directly from Word, Excel, or some other form. The hardest are forms that have been scanned into a computer (often out of order). OCR software can be expensive. If all you want to do is search, you can use DocumentCloud. That service automates the use of the free Tesseract engine. But beyond that, you’ll probably have to pay for a tool. See a list below. ##Searchable tables PDFs are designed to preserve the format on the page. You shouldn’t think of them as data files but as printed reports. Sometimes copying and pasting leaves you with unintelligible line breaks and smushed-together cells. You’ll usually have to use some kind of tool to deal with this. The key to trying to convert PDFs is to try every tool you can afford. You’ll never know which one works. Again, these will only work with pdf’s that are already searchable. Free tools CometDocs is a free service that lets you upload your pdf and will return to you an XL file in email. Your IRE membership gives you a paid version for free, which lets you keep all of your pdf’s on their site and puts you in the front of the line. Tabula, a new alpha-level project from ProPublica and the Mozilla fellows (and also Jeremy Merrill now at the NYT). It lets you show the program where a table is, and it does a little more sophisticated work to figure out where the cells are. It’s a little better with PDF’s with lines between rows than CometDocs. xpdf is an old but reliable command line tool that still works remarkably well. It’s often best for very fixed-width and consistent pdfs. It’s a bit of a pain to install on Unix-based (including Mac) systems, but once you have it it’s easy. See this tutorial from IRE to use it. You’ll have to log into IRE to get it. PDF Tables from ScraperWiki is free for now. It may restrict large-scale conversions. It uses a different algorithm that might sometimes preserve the position of cells best. There are also PDF plug-ins to Ruby and Python, which can sometimes work just fine. ###When free isn’t good enough Most of the pay products have two levels: one if you don’t need to OCR, and another if you do. If you’re going to pay, you might as well get the OCR. It’s not that much more and you’ll eventually need it. Sometimes it’s worth seeing if you can pay just a little more than that for what’s called “batch” conversion, or “hot folders.” That means that when you get 10,000 pdfs zipped up into one file, you won’t have to individually convert them all. Rob Barry created this tutorial using ABBYY to extract information from even the most difficult forms. *Able2Extract: $99.95 for non-OCR/ $129.95 for OCR. Intuitive and reasonably accurate. It has some annoyances, but saving as a text file often overcomes them (rather than going directly to Excel). *ABBYY FineReader Also $99.99 for personal use, available for Mac and Windows. Includes OCR. *OmniPage, $149.99 for individuals. This one is just slightly more powerful, but I’ve had better luck with it over time. The constant upgrades are really a pain. (Windows only) *Cogniview has PDF2XL, which has been a treat for very difficult pdfs. It has a hard learning curve and is only really useful on tables, unlike OmniPage and ABBYY. ($129.97). (Windows only)",
    "url": "http://localhost:4000/cronkite-docs/special/cleanup_pdf.html",
    "relUrl": "/special/cleanup_pdf.html"
  },
  "6": {
    "id": "6",
    "title": "Cronkite-Mayo",
    "content": "Cronkite-Mayo Clinic data reporting May 2018 Presentations Powerpoint from class - Becoming a better watchdog The AP’s Meghan Hoyer’s health care Datapalooza from the USC health care fellowship, 2017 (with permission). Her tip sheet on sources is made for California reporters, but it will give you a sense of what’s available elsewhere. Data compilations Look through ProPublica’s data store for some cleaned datasets and links to other government sources. Start with “Vital Signs” data lookup, then consider learning how to use its API. The mother of all regulatory sites: reginfo.gov. Search to the “Information Collection” section. To reduce the number of results, use only “active” forms. (Video to come). While you’re there, check out the “Unified Agenda” for any agency you care about. This is the list of upcoming regulatory and deregulatory actions, created for lobbyists to know what to expect in the future. healthdata.gov is supposed to be a compilation of all HHS datasets. Look at them by component for the easiest navigation. CMS datasets (HHS’s Centers for Medicare and Medicaid Services) CMS’s data sources are confusing. They could be found in cms.gov, medicare.gov or medicaid.gov. Standard provider level datasets Medicare utilization public use files Medicare consumer-oriented datasets on quality and selected utilization at https://data.medicare.gov/data openpayments: Drug company and device payments to providers Cost reports for Medicare-eligible institutions, including hospitals, nursing homes, renal clinics, home health care, and hospice. These are the reports that show whether they are making and spending enough to provide care. It includes general financial information on all of the institutions’ activity, not just Medicare patients. Drug price and utilization CMS drug spending has average prices paid in Medicare Part D, Part B and Medicaid. It also has the most recent year total spending. data.medicaid.gov has more details on drug utilization. Nursing homes A lot of people use the CMS’s Nursing Home Compare tool and data, but Propublica has made it much easier: Use its Nursing Home Inspect tool, with downloadable data of unredacted inspection forms. It has documentation on sourcing and the meaning of the files. The “full statement of deficiencies” for April 2018. Look at the bottom of https://www.cms.gov/medicare/provider-enrollment-and-certification/certificationandcomplianc/fsqrs.html to see a newer link. (The “download” directory isn’t public, so you can’t just tour it.) A list of state contacts for nursing home inspections and regulations.",
    "url": "http://localhost:4000/cronkite-docs/workshops/cronkite-mayo.html",
    "relUrl": "/workshops/cronkite-mayo.html"
  },
  "7": {
    "id": "7",
    "title": "Data diaries",
    "content": "The data diary At the Associated Press, data reporters issue a simple command when beginning a project, which sets up a standardized set of files and folders. From there, reporters’ work is centrally stored and documented in standard locations, making it easy for any one on the team to dip in and out of the project. All of the unit’s work, including its story memos, are done using standardized tools that allow for replication at any point in the project and ensure that any communication with all members of the reporting and graphics teams are looking at the same, up-to-date results. “We have the one-person bus rule,” said Meghan Hoyer, the team’s manager. If someone is hit by a bus, someone else should be able to pick up the project without wasting any time. One concession Hoyer made was to standardize the team around the R programming language. She doesn’t regret it. “It was a big lift at the time,” she said. “But now I could never go back” to overseeing projects done using less formal structure and documentation. (Note to students: AP data journalism interns are required to come with some facility in R, another good reason to learn it.) Replication and the data diary It doesn’t matter where you come down on the esoteric debate on replication. Your data, its analysis and the way it’s characterized in publication must be demonstrably accurate. That means understanding exactly what you did, why, where it all is and how it should be communicated to a general audience. The formal processes used by AP might not work for smaller endeavors, but anyone can put the underlying ideas to work. At the Center for Public Integrity, Talia Buford kept a simple Word document with her questions and code annotated to help her repeat her work. Enter the data diary. Think of the data work the same way you think about interview notes or transcripts and any other research for a story. You wouldn’t quote a court case without reading it and probably talking to some of the participants. You’d make sure you know where the documents are, and what people say about them. All data work should be documented in at least the same detail. Ideally, someone reading through your notes would be able to repeat your work and understand what it means. This is a note to your future self. Things happen in newsrooms. Stopping work halfway through a story and picking it up again six months later happens all the time. You should be able to pick up where you left off after briefly refreshing yourself on your work. In this module you’ll learn: The kinds of information you need to record in a data diary or in your documentation The questions you’ll need to be able to answer before you publish A format for recording your work. Getting started Before you even start work on a new dataset, open up a new document and just start writing down what you do. For a quick daily story, you might be able to keep your work in one short document. For a longer project, you may find it easier to break your documents apart into logical pieces. In many cases, you can link to a Jupyter notebook or R markdown program that is self-documenting. In others, you’ll need several documents to maintain a log of where you got data, who you spoke with and interviewed, what alternatives you looked at and what you decided along the way. Here are some sections that are worth considering whenever you start a story or project. Data sourcing Where did you get the data? How do you know it’s authentic? A description of who originally collects the data and why it exists. List of other stories that have used this or similar data – links, advice, warnings and findings. Academic work that has relied on the data. Names, contact information for those sources. Alternative sources for this and similar or related datasets or documents. Data documentation and flaws You can keep editing this section as you work on the problems in the data. If you found some 10-year-old children with drunk driving records, you either imported or converted the data badly or there is a mistake in how it was recorded or transmitted to you. Once you’ve found out why and fixed it, you can nix that from your diary. But if it’s a flaw in the underlying data, you’ll describe how you tried to resolve it and what you decided to do about it. List each step you took to look for flaws. For example, “Filtered on each field and looked for missing data.” – then what you found. What integrity checks did you make? How were they reconciled? Do your totals match reports generated by an agency? Why not? Are there 10-year-old children with drunk driving convictions? How did that happen and how will you resolve it? Links or copies of any original documentation such as a record layout, data dictionary or manual. If there isn’t one, consider making a data dictionary with what you’ve learned. Questions about the meaning of fields or the scope of the data. Decisions you’ve made about the scope or method of your analysis. For example, if you want to look at “serious” crimes, describe how and why you categorized each crime as “serious” or “not serious.” Some of these should be vetted by experts or should be verified by documenting industry standards. A list of interviews conducted / questions asked of officials and what they said. Processing notes Some projects require many steps to get to a dataset that can be analyzed. You may have had to scrape the data, combine it with other sources or fix some entries. Some common elements you should document: Hand-made corrections. Try to list every one, but it’s ok if you describe HOW you did it, such as clustering and hand-entering using OpenRefine. Link to any spreadsheet, document or program you used. Geocoding. Note how many were correct, how many missing, and what you did about it. A description of how you got messy data into a tabular form or a form suitable for analysis. For example, you may have had to strip headings or flip a spreadsheet on its head. Make sure to write down how you did that. The good part: Your analysis Each question you asked of your data, and the steps you took to answer it. If you use programming notebooks, write it out in plain language before or after the query or statements. Vetting of your answers: who has looked them over, commented on them Why they might be wrong. Examples of documentation A published Jupyter notebook for an analysis of FEC enforcement actions from the Los Angeles Times’ data desk. Ben Welsh, the author of that notebook, says that there are previous versions with unpublishable work. An example of a data diary kept by Talia Buford, a ProPublica reporter, when she was working at the Center for Public Integrity. She’s nicely annotated the document to show why she structures it the way she does. A 2018 Buzzfeed News repo with start-to-finish documentation of an opioid deaths story. Data smells: looking for flaws in data We’ll have a module on data cleaning, but for now, here are a couple of readings that talk about dirty data. An email exchange between Sarah Cohen and Craig Silverman, now at Buzzfeed, about the process she used at The New York Times in reporting and fact-checking. This isn’t the same as a process for replication, but it discusses the kinds of things that should be in it. Ensuring Accuracy in Data Journalism, or Data Smells, by Nikolas Iubel – Sarah Cohen, 2018",
    "url": "http://localhost:4000/cronkite-docs/general/data-diary.html",
    "relUrl": "/general/data-diary.html"
  },
  "8": {
    "id": "8",
    "title": "Designing your own database",
    "content": "Building your own database involves keeping tidy data concepts in mind while minimizing the amount of work that you’ll have to do to fill it out. In other words, you should be conscious of how you might want to use the data while at the same time being realistic about what you can reasonably get. There are a few common reasons to design and build your own data for stories: There is a long-running story that is periodically updated with new documents or events that you want to keep track of, including background information. An example might be a bankruptcy that drags on for years, or a person who is continually in the news. One reporter in the ‘90’s created a spreadsheet to log each event related to Jack Kevorkian, a famous doctor who helped people commit suicide. You are getting information from disparate sources and you need an easy way to search them, arrange them chronologically, and keep track of what you need to verify. Think of your own database in four ways: Unit of analysis - there may be more than one you care about Ease of data entry Statistical summary vs. narrative Fact checking and data sourcing The process of building your own database is somewhat iterative. You start with what you think you want, then reevaluate after you’ve tried 5 to 10 entries. Some examples Chron Unit of analysis Your first choice is the unit of analysis you want to record.",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/design-database.html",
    "relUrl": "/assets/docs/design-database.html"
  },
  "9": {
    "id": "9",
    "title": "Arizona population exercise",
    "content": "Instructions Data Documentation Source Changes to the file Reference map Instructions This spreadsheet contains figures for the estimated population of Arizona counties from 2012 through 2017. It also contains the components of change. Your job is to find a story in this data by using change, percent change, percent of total to find something newsworthy. Consider looking up some stories written on similar data, either in Arizona or elsewhere, to see how reporters generally treat population data releases. One example, based on data released by the state, is from Tucson.com. As you do your analysis, consider the unit of analysis (county) and what the various components might tell you. Did a county grow because of migration from other areas, or because there was a relatively high birth rates? Is the county dying off? Make sure to read the documentation and any definitions you can find before you start. Data Documentation Source The dataset was derived from the U.S. Census Bureau’s annual estimates of population of the U.S., which breaks down the change in population based on its components: Natural population change, which is births - deaths; migration, which includes net increases in immigrants from other countries (international) and other counties (domestic); and what it calls a “residual”, which is an unexplained portion of the estimated change in population. The data was downloaded from the Census’s county population site at https://www.census.gov/data/datasets/2017/demo/popest/counties-total.html. The csv file is at the bottom of that page under “Datasets”. The Census bureau uses many sources to compile these estimates, but they are not the same as a national census. The sources include vital records from state and local health department on birth and death certificates; estimates of domestic migration from tax and other records, and estimates of immigration from other countries from a variety of sources, including schools. They then try to make it all add up to the annual American Community Survey estimates at the state level. The full methodology is linked off of the main page. Changes to the file Arizona counties and the state total were pulled out of the national file. The data was kept for the years 2012 to 2017 to get a five-year change estimate. Populations are kept for all of the years. Net change in population, births, deaths, natural increase, international migration, domestic migration, net migration and residuals are the totals for the years 2013 through 2017. The data was checked to makes sure that: births + deaths = natural change international + domestic migration = net migration natural change + net migration + residual = population change population 2012 + population change = population 2017 Reference map It will be easier to work with the data if you have a sense of Arizona geography, especially which counties contain the large cities and which counties are made up of tribal lands. Green areas are national forest and parkland; striped areas are reservations.",
    "url": "http://localhost:4000/cronkite-docs/excel/practice/excel-azpop-exercise.html",
    "relUrl": "/excel/practice/excel-azpop-exercise.html"
  },
  "10": {
    "id": "10",
    "title": "Arizona Diamondbacks scores",
    "content": "Pivot tables: How many? How much? Files for this tutorial: Grouping Counting baseball games Summarize your data Different kinds of summaries Automatic percentages More complicated pivot tables Crosstabs On your own A pivot table is a powerful way to summarize data in Excel. It’s designed to answer questions that start with the words, “how many?” or “how much?”. Before you start, review the tutorial written by Matt Waite of the University of Nebraska to get a basic understanding of summarizing data using pivot tables. Download the .csv file for that tutorial by right-clicking on this link and choosing “Save Link as…” (in Chrome) or “Download Linked File As..” (in Safari) Files for this tutorial: Arizona Diamondback scores for 2018 This file has four pages: games: The main data page showing the results the Arizona Diamondbacks’ 2018 season, including the win-loss record, scores, attendance and other details. This data is formatted as a data table, and follows tidy data principles. team_names: A translation of the abbreviated team names, to be used in a different exercise. source: The source of the data with a glossary of terms diary: Changes made to the original to prepare for this exercise Grouping Every data analysis program can do some form of “grouping”. Many reporters confuse “sorting” with “grouping” because that’s how we express it in plain language. But in Excel, sorting and grouping are very different things. Sorting involves arranging a table’s rows into some order based on the values in a column. In other languages, this is called arranging or ordering, much clearer concepts. Grouping, on the other hand, means to summarize – count, sum, or average, for example – using the value of a discrete categorical value. Examples include: The number of crimes each month The total value of campaign contributions in each state The average height of men vs. women in your class The simple pivot table tutorial shown above displays the number of mountain lion sightings in each Nebraska county: Counting baseball games Suppose you want to know how many games the Diamondbacks won, and how many they lost in the 2018 season. You could filter the original table, note the number of items shown, and then filter the opposite way. But then you’d have to repeat that every time you wanted to check your answers. Instead, it would be much better if we just had this table: Win 80 Lose 82 Total 162 That’s exactly what a pivot table does. Summarize your data There are several ways to get a new pivot table: If your data is formatted as a table, as the Diamondbacks worksheet is, you can select the “Summarize with a Pivot Table” button under the Table tab. Select any single cell in your table, and choose Insert, then Pivot Table. Use the menu (not the ribbon), under Data, Summarize with Pivot Table. ßß Excel will guess what data you want to summarize – check to make sure it’s right (in our case, Table 2), and then select where to put the table. You’ll usually leave it on the default, which is to make a new page in your workbook. Now you can pour your data into the empty shell. Drag the fields you want from the list into the empty boxes. Anything you put in the “Rows” section will be shown in rows, down the left side of the table. These will usually be categories, names or dates. The most confusing part of this box is the “Values” area. This is the number that will be shown in the middle of the table – the one you want to compute. If you choose a field that contains words, Excel will guess that you want to count (Count) them up, answering the question, “How many?”. If you choose a field that contains numbers, it will assume you want to add them up (Sum), answering the question, “How much?”. Different kinds of summaries You can compute more than one number. For example, this table shows you that almost 5 million tickets were sold over the season, averaging 29,760 per game. And that they averaged higher attendance when they lost than when they won. Automatic percentages Automatic percentages are computed using the same box, but in a slightly different form. Drag the number field into the Values area again, but this time click on “Show Data As…”, and, in this case, choose “Percent of Column Total”: {: .m-3}) More complicated pivot tables You aren’t stuck with using only one field – you can have as many as you can fit on the page. Here’s an example, showing win-loss statistics at home and on the road, at night and during the day. Crosstabs You probably ran into crosstabs when you studied statistics, or in a political science course if you analyzed poll results. The idea of a crosstab is to compare two (or more) variables in two dimensions: columns and rows. At the simplest level, it’s just a count of how many items fall into each category. In this case, let’s look at how many wins and losses the team had playing at home and away. Just drag at field into the Column box to get a 2-dimensional table, grouped by both home/away and win/lose. It happens that the Diamondbacks won just over half of the games they played, which were split equally between games played in Phoenix and games played elsewhere. These numbers look like the team does a little better on the road, but it’s hard to be sure. That’s where percentages - or rates - come into play. We’d like to be able to say whether the team is more or less likely to win at home or on the road, and by how much. These are common kinds of questions in journalism and in life. For example: Do immigrants get deported at higher rates when they have criminal histories? Are women more likely to get c-sections when they have private health insurance? Are high-income minorities rejected for loans at a higher rate than non-minorities? Are you more likely to score well on an exam in the morning or afternoon? Translating these questions into pivot tables takes practice and you often have to read your results out loud to yourself. At first, just use Philip Meyer’s rule of thumb: Put the category that happens first in time (the independent variable) in the column area, the category that comes later in the row area, and compute the column percentages. In our example, it will look like this when you’re done: On your own Try to think of other statistics or comparisons you’d like to calculate by group in this dataset. For example: Do the Diamondbacks play better at night or during the day? What is the longest game, in innings, played against each team? What is the win-loss record against each team?",
    "url": "http://localhost:4000/cronkite-docs/excel/practice/excel-baseball.html",
    "relUrl": "/excel/practice/excel-baseball.html"
  },
  "11": {
    "id": "11",
    "title": "Swimming pool inspections",
    "content": "Grouping from start to finish This practice example goes through the kinds of work you have to do before you can even start filtering and grouping. It’s the “reporting” side of data reporting, even if it often means exploring data and reading. Interviewing your database Look for a guide on the agency website What do you have? How does what you have compare to what the agency shows? Start asking questions Going further TK - Filtering your data TK - Grouping with pivot tables This excercises uses a database of swimming pool inspections obtained by Courtland Jeffrey at ABC15 through a public records request to the Maricopa County Environmental Services department. This website reviews exactly which pools must be inspected. Interviewing your database Look for a guide on the agency website Before you open the spreadsheet, review the material available on the county’s website. Read the information available on what the department does, and then click on the Swimming Pool Inspections link to the left. The Weekly Report simply provides a list of the inspections conducted for a single week. Note that you can also search for a business name or address. This report should show you what each column in your spreadsheet means. What do you have? Now open your spreadsheet and start interviewing your data. Use a filter to help you answer these questions: What makes this dataset “tidy”, or, alternatively, in what way is it “untidy”? Note what data type is in each of the columns – are they codes, categories, numbers or free text? Is the text standardized? Are there comments of the kind you would fill into a big box on a form? What is the date range of this dataset? What is the unit of analysis, or can’t you quite tell? Note that this spreadsheet is formatted as a “data table”, which means you have the option of making formulas based on the column heading or based on their cell addresses. How does what you have compare to what the agency shows? If you look up a few of the addresses that you see in your spreadsheet, you can make sure you know what each column heading means by matching the values against the online version. You can also know a little more about what was NOT provided in this dataset under this request. Note that it’s not always easy to find the right pool by looking up the name and address. For example, the Union Hills Estates “Main” pool doesn’t appear in the list of pools under that name or address. There are other pools, but none with the same Permit ID as the ones in our database: Let’s see if we can trick the Web app into giving us the proper pool to compare. In this case, try clicking on one of the existing pool numbers, then look at the URL in your browser: Change the url to one in your spreadsheet, then filter your spreadsheet for just that pool: Two of the inspections that are in the date range of our dataset are shown on this list: 2/9/2016 and 10/6/2016. Take a closer look at them and compare what is in your spreadsheet to what is on the web. If you click on the other inspection that you found in our database, you’ll see that, eight months later, a routine inspection showed that two of the violations are repeated, and that the county did nothing. On your own, look up some other interesting items that you found on your database – try looking at different kinds of inspections, different outcomes, and different statuses to see how they are reflected on the web. Jot down two or three story ideas that might emerge from an analysis of this, and two or three that would emerge if you had the data the way the county holds it – in a proper database rather than as a summary. Start asking questions Using your newfound deep understanding of swimming pool inspections, try answering these questions: Which properties have had the most violations? Find the most common violations. Which violations lead to pool closures? There are some things that will be hard without flagging some of the data – for example, if there was no violation, the address is still included. For that reason, you may want to add a column at the end that tests for whether there was a violation. In this case, we can say that this is an INSPECTION record, if the violation is blank. Otherwise, it’s a VIOLATION record. Going further Create a unique identifier for each inspection, then group by that number to find the number of INSPECTIONS by type, rather than the number of VIOLATIONS or RECORDS. One way to do that is to use the numeric value of the dates to combine with the swimming pool ID. You can turn the date back into a number using the formula: =TEXT([Inspection Date],&quot;00000&quot;) To put two text fields together, use the &amp; symbol: =[Permit ID]&amp;TEXT([Inspection Date],&quot;00000&quot;) Now you can create a Pivot Table to show a row for every inspection: TK - Filtering your data TK - Grouping with pivot tables Congratulations! You’re now a data reporter!",
    "url": "http://localhost:4000/cronkite-docs/excel/practice/excel-pivot-pools.html",
    "relUrl": "/excel/practice/excel-pivot-pools.html"
  },
  "12": {
    "id": "12",
    "title": "Use of force database",
    "content": "Desigining your use of force database This exercise will start with some reading on police use of force, specifically the use of force using guns. There have been many stories about the more specific issue of fatal police shootings, but all shootings require you to get data from local sources. In this exercise you will: Read a few stories that attempted to document police shootings and evaluate where they succeeded and where they came up short. Analyze a database produced by the Phoenix Police Department’s open government initiative, created after community groups decried the lack of information available. Using the government version as a base, design a new record layout and structure that you would collect using documents and news stories that will fill in some of the missing information Getting ready “Shot by Cops and Forgotten” by Rob Arthur, Taylor Dolven, Keegan Hamilton, Allison McCann, and Carter Sherman, VICE News, December 2017 “How Phoenix Explains a Rise in Police Violence: It’s the Civilians’ Fault”, by Richard Oppel, The New York Times, Dec. 10, 2018 “Data on Use of Force by Police across U.S. Proves Almost Useless”, by Matt Apuzzo and Sarah Cohen, The New York Times, Aug. 11, 2015 The FBI’s new use of force data collection page, showing what data elements it believes local agencies should be able to report. Look at how the Orlando Police Department releases its use of force data (a csv copy saved here, and an example of a use of force report linked from that database. Several longer documents would be useful if you were going to do this for a real project. For example, the Maricopa County Attorney’s Office investigates every officer-involved shooting and publishes a list of their conclusions with names and decisions. (It does not decide whether police broke the rules. It only decides whether to prosecute the officers under normal criminal laws.) The Phoenix Police Department’s 1,200-page operational orders have a long section on the department’s use of force policy. The department also has a Use of Force Board that determines whether police broke the law, but does not publish those documents on its website. Open data The current data is updated on the Phoenix Open Data site in a comma-delimited text file. A copy of that data was saved as of Dec. 24, 2018, in case it is taken down. Import the data into a spreadsheet and evaluate it by sorting and filtering – you don’t need to know anything more than this right now. Answer the following questions: How many incidents were logged in 2018? Using the filter checkboxes, check weekdays vs. weekends counts. How many did not involve a firearm brandished by the suspect. Design your dream data Now create a data dictionary for a new dataset that would be based on this, but would contain what you really want to know about the cases. Some are obvious like the names of the officers and the suspects and a narrative of the incident. But think clearly about what you need for a published story – include items that will help you fact check or allow you to count less obvious items like the police officer or supervisor responsible for the most shootings, or a tag for the circumstances. Make sure you build in fields that will help you fact check, such as a URL for the source document, or a yes-no field for name spell checks. Your document should have the following information: Name of the field, in standard data form. A description of the field – what does it mean? How are edge cases like multiple officers handled (eg, semicolon separated, or listed on a separate table)? Source of the field – where will you find it? Are there first, second and third choice sources? How you expect the field to be used, and what happens if you can’t get it. Using open source resources such as police or county attorney press releases or other news reports, try filling out five records.",
    "url": "http://localhost:4000/cronkite-docs/excel/practice/excel-police-uof.html",
    "relUrl": "/excel/practice/excel-police-uof.html"
  },
  "13": {
    "id": "13",
    "title": "Rates and ratios with FBI data",
    "content": "Reading for FBI crime reports: FBI crime statistics We’re going to look at Uniform Crime Reports for 2016 in class. Data and documentation I downloaded the data from the FBI for 2017. The files are: Original FBI data. Read the “Data Declaration” at the top of this page. The original data was downloaded from the “Download Excel” link at the top of the page. A cleaned-up version of the dataset for us to use while you’re reading about the UCR. (NOTE: I’ve changed this since class. I eliminated the first crime rate and took out the property and violent crime totals, which is the way the FBI gives it to us.) It contains three pages : Original_data: This was the full list of cities that the FBI reported. Be sure to review the footnotes at the bottom, which will explain some anomalies you find later on. You should note that it contains three types of numbers: A population for the city. Total numbers of violent and property crimes, which should be the sum of the parts, except for Arson. Each “Part I” type of crime. These are the specific crimes that local police agencies are asked to report to the FBI each year for this program. A somewhat (but not entirely) tidy version of the data, but only for large cities. A list of Arizona cities. Stories that were based on UCR crime data It’s always worth the time to read how others have presented a common dataset so you don’t miss important nuances. “Reports of Rapes and Hate Crimes Rise in 2018, but New York Remains Safest Big City,” New York Times, Dec. 31, 2018; “As the number of Reported Rapes Climb, Mayor Points to #MeToo”, New York Times, Jan 6, 2019 “LAPD misclassified nearly 1,200 violent crimes as minor offenses”, Los Angeles Times, Aug. 9, 2014 “[FBI crime report shows national decline, but uptick in metro-Phoenix]”(https://www.azcentral.com/story/news/local/arizona/2018/10/01/fbi-crime-report-shows-national-decline-uptick-metro-phoenix/1433323002/) Arizona Republic, Oct. 1, 2018 Check the government’s math Get in the habit of trying to reproduce numbers that were calculated by others. In this case, you’ll want to check to see if you get the same number of violent and property crimes, based on the FBI’s definitions in the data declaration. Calculate the number of violent crimes. Calculate the number of property crimes, excluding Arson. You’ll notice that not all of the numbers match. Returning to the original sheet and looking for footnotes will help explain it.",
    "url": "http://localhost:4000/cronkite-docs/excel/practice/excel-rates.html",
    "relUrl": "/excel/practice/excel-rates.html"
  },
  "14": {
    "id": "14",
    "title": "Excel",
    "content": "",
    "url": "http://localhost:4000/cronkite-docs/excel",
    "relUrl": "/excel"
  },
  "15": {
    "id": "15",
    "title": "File formats",
    "content": "Understanding file formats Text-based formats CSV, TSV and other delimited data files Benefits Traps Fixed-width text XML and JSON Benefits Traps HTML Traps SQL dumps Proprietary file formats Excel, Access and their cousins PDF Non-text files Image, sound and video Vector images Data doesn’t always show up in a simple Excel spreadsheet or on a Google sheet. There are about a dozen common forms that you’ll find data. File types refer to two different things. At its core, data could be: Text - the (nearly) universally recognized format that can be opened and managed in any software. There are different encodings, but usually you’ll find them labeled as UTF-8 or ASCII. Don’t worry about it for now. Binary - machine-readable but encoded in computer-ese, often in proprietary form. Images and sound are always binary, but so are PDF files. Both of these types, at their most basic level, are transmitted using bits – a series of 0s and 1s – but text file bits represent characters and binary file bits represent some other kind of data, either proprietary or non-character. The other kind of file specification tells you how it is organized – in rectangular table structure, or in some other form that looks more like documents? Most file structures are denoted by their file extension, or the part that comes after the period. In both Macs and Windows, you may not see those extensions by default. Make sure you see the extensions on a Mac by opening a Finder window and choosing “Preferences.” Make sure the box that says “Show all filename extensions” is checked:    In Windows, use the shortcut for All Settings in the bottom right, then search for File Extensions. A window will pop up and make sure that “Hide extensions for known file types” is unchecked. Text-based formats CSV, TSV and other delimited data files Tabular text, the simplest data to read into any program, can come in several structures. The most common is called CSV, or comma-separated values. Your computer probably thinks that a CSV file is supposed to be opened in Excel or, on a Mac, in the built-in Numbers program. A CSV file suggests that a program wishing to use the data table should use commas to separate the columns. Usually, the data is enclosed by double quotes when it’s possible to have a comma within the field. The first row usually shows the headings in the same form: name, position, start_date, age_at_start_date &quot;Trump, Donald&quot;, President, 2017-01-20, 70 &quot;Obama, Barack&quot;, President, 2009-01-20, 47 If you see a file like this on the Web, you can open it in your browser and safely save it with a .csv extension so that other programs recognize it, but it’s not necessary. You can use it no matter what it’s called. Some people use TSV instead of CSV, replacing the comma with a TAB key. The reason is that it’s less likely you’ll need to use the quotes, which then reduces some confusion when your data could include quotes. They are much less likely to include TABS. Benefits CSV and its cousins are the international language of data. They work with every open source product and every piece of brand-name software. Traps Excel and Google sheets quite annoyingly like to think they know what’s in your data, and don’t let you specify the way the fields are read. This means that if it sees a number, it will treat it as a number even if it’s a Zip Code. It will also turn fields that look like dates into dates, even when they shouldn’t. This comes up frequently in two places. In anything related to schools that list the grades that school includes, you’ll often see ‘9-12’ turned into September 12th or September 2012. In some cities, addresses with dashes also get corrupted, turning 12-21 into December 12th. (This is very frequent in Queens, NY) These are very specific problems, but they are impossible to fix if you lose the original CSV file. You can usually just change the extension from .csv to .txt and then you’ll be allowed to specify what you want the program to do. The other trap is harder to deal with and is treated differently in each program – that’s the problem of unescaped quotes and line endings in long fields. An unescaped quote would look something like this: Donald &quot;You&#39;re fired!&quot; Trump, president, ... In this case, programs won’t know what to do. Some, like Python, try to be smart – it works fine if there’s o comma directly after the internal quote. Others completely fail. Even worse, different programs expect different “escapes” for internal quotes. In MySQL and some other programs, a backslash tells the program to ignore the upcoming quote. Microsoft products usually want two consecutive ones: Donald &quot;You&#39;re fired! &quot; Trump Donald &quot;&quot;You&#39;re fired!&quot;&quot; Trump This is a common problem when someone has exported the data into a text file from some proprietary format like Excel. Fixed-width text This is another form of a universal file format that can be read by anyone, except it splits the fields at specific locations within a row rather than using specific characters. These are a little harder to read, but are common when you get data produced by old computers in government agencies. The bigger disadvantage is that every field has to be the same size. If someone has a long name or title, for example, it will be chopped off in this format. You will usually need a data dictionary or record layout to tell you how to split it apart. Trump, Donald president 2017-01-20 Obama, Barack president 2009-01-20 Biden, Joe vice president2009-01-20 Washington, Georpresident 1779-04-30 XML and JSON XML (eXtensible Markup Language) and JSON (Javascript Object Notation) are two other universal ways to share plain text data, but they work in a very different way from the tabular information we see. In both cases, they define a structure and can have multiple pieces of information within a record. They are both tagged, in that each field is named whenever it’s used. At its simplest, XML looks a lot like HTML (below), except it uses field names and characteristics to do its work: &lt;employees&gt; &lt;employee sex=&quot;M&quot;&gt; &lt;firstName&gt;John&lt;/firstName&gt; &lt;lastName&gt;Doe&lt;/lastName&gt; &lt;/employee&gt; &lt;employee sex=&quot;F&quot;&gt; &lt;firstName&gt;Anna&lt;/firstName&gt; &lt;lastName&gt;Smith&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Peter&lt;/firstName&gt; &lt;lastName&gt;Jones&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt;ß At one time, XML was the official format required by federal agencies for data transfer, so there are a lot of government-generated XML files. JSON also defines the structure of a dataset by using the names of the fields, but it’s stored in a somewhat more compact structure: {&quot;employees&quot;:[ { &quot;firstName&quot;:&quot;John&quot;, &quot;lastName&quot;:&quot;Doe&quot;, &quot;sex&quot;:&quot;M&quot;}, { &quot;firstName&quot;:&quot;Anna&quot;, &quot;lastName&quot;:&quot;Smith&quot;, &quot;sex&quot;: &quot;F&quot; }, { &quot;firstName&quot;:&quot;Peter&quot;, &quot;lastName&quot;:&quot;Jones&quot; } ]} Leaving out a field simply leaves it blank in the output dataset. Most Application Programming Interfaces (or APIs) return their data in .json format, which is easily converted into a complex object by javascript, Python or R. JSON was designed for efficiently transferring data to your browser, so it’s usually the most flexible format. Excel doesn’t open json by default, but Google sheets can easily import simple json objects. For both XML and json, look for online converters to .csv files online. They won’t handle large documents, but they’re perfectly good to convert files with a few hundred items in them. Benefits No one has to guess what anything means in these tagged structures. They can include anything you want, including full documents within one field. Once a computer program can read one of these formats, there are very few ways they can go wrong. Most computer languages such as R or Python have standard methods of reading these files, making them relatively easy to work with. Don’t curse these formats. They’re your friends. Traps They’re often NOT properly created, leading to frustrating “malformed” errors in programs that can read them. Complicated structures – ones with nested pieces of information, such as a list of nicknames for each of these employees – and very large files are almost impossible to open in software without reading it through a program first. HTML At its core, HTML is a structured document containing text. Images, sound, videos and other files are linked to them; the formatting you see is done through text instructions. That doesn’t mean it’s easy to work with as a data source. You’ll have to parse the document to get its pieces out. Traps Even when you do get the text out, there are often weird entities that are used for things like accents and other characters. For example, instead of a dash, “–”, you’ll see “&amp;mdash;” or instead of an “&amp;”, you see “&amp;amp;”. This is particularly annoying in lists of names with accents and other non-standard characters. Converting between the various text encodings can also be a headache, but it’s getting easier. (Old versions of Python choke on UTF-8 characters, like accents or smart quotes.) SQL dumps A SQL dump is plain text or binary information that is designed to be read back into a relational database program like MySQL, SQLite or PostGreSQL. While not strictly text, it is considered an open format. (SQL databases can store more than text, including images or sound, but it’s stored in a standard way.) Just know that if you need to ask for data, a SQL dump is a very easy way to work with a large database, and is usually easy for a government agency to produce. Proprietary file formats Most software saves files in special formats that they and only they can read. Some are so common, though, that there are converters and importers, or they have become de facto standards. Excel, Access and their cousins These are files the files created by software that contain special instructions in computer code that are intended to be read by the originating programs. Many of the formats you’ll see originated with Microsoft: .xlsx, .accdb, .docx and .pptx. In fact, Google docs downloads, by default, are converted into these Microsoft formats. They are much more susceptible to viruses and other malware, but they can contain a lot more information. They can be either text or binary. In fact, Microsoft Excel and Word documents are now text files – very, very, complicated text files that you wouldn’t want to try to decipher on your own. PDF I’ve saved PDF because it’s the bane of most data journalists’ existence. PDF files were designed to present information on a page exactly as the person who created it wanted. It’s really a report, not a dataset. But many government agencies now try to pawn off PDFs as data. There are all kinds of issues with PDFs, but for now just leave it this way: You don’t want them. This list suggests that, depending on the complexity of the data you are requesting, you’ll usually want one of the standard text-based formats: csv or tsv for simple data; xml or json for complicated data. Non-text files Image, sound and video There are two kinds of binary files. The first is a set of files that are standard across all operating systems and used to convey images, sound and video. These are .jpg, .png, .mp4, .mp3, etc. You can’t read them with a text file, but almost any program that processes that kind of file will open and save them. You can read them into computer programs and work with them bit-by-bit. For example, many machine learning algorithms are used to identify pictures that are the same. They look for codes that tell them what each pixel of the screen shows. Each pixel can be one of about a million different colors. We won’t be dealing with these much, but it’s worth it for you to know that you can do analysis of sound, video and image just the same way you can with tabular data. Vector images These files are often used to transfer images made with Adobe Illustrator or in mapping programs. Rather than put each pixel in the file, each object – a dot, line, or polygon – is defined by a giant mathematical formula. It’s more efficient than working with pixels, and it allows the objects to contain additional information. Shrinking and enlarging them doesn’t change the file size or the quality.",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/file-formats-orig.html",
    "relUrl": "/assets/docs/file-formats-orig.html"
  },
  "16": {
    "id": "16",
    "title": "File formats",
    "content": "Understanding file formats When you encounter data in the wild it won’t always come to you in prepared Excel files or Google sheets. In fact, it’s become more common for data providers to post their information in a more universal form that can be more easily read by many different programs, not just spreadsheets. This should make you happy, not sad. We’re going to skip over the more technical side of the concept of data formats, and just go through what awaits you when you find data on the Web. Most file structures are denoted by their file extension, or the part that comes after the period. In both Macs and Windows, you may not see those extensions by default. Make sure you see the extensions on a Mac by opening a Finder window and choosing “Preferences.” Make sure the box that says “Show all filename extensions” is checked:    In Windows, use the shortcut for All Settings in the bottom right, then search for File Extensions. A window will pop up and make sure that “Hide extensions for known file types” is unchecked. tl;dr Best : csv and other tabular text, json, xml, and any spatial data So-so : Excel (xlsx), html Avoid : pdf tl;dr Text data Tabular data Common file formats CSV, TSV and other delimited data files Benefits Traps Fixed-width text XML and JSON Benefits Traps HTML Traps Proprietary file formats Excel, Access and their cousins Benefits Traps PDF Spatial data Traps Text data A dataset is considered “text” if it used only to convey characters you can type on a keyboard. Text-based data can be opened and read by a human in any text editor, like Notepad on Windows or TextEdit on a Mac. Most data journalists have a powerful text processor on their computers so they can easily check any file that comes across their desks. Examples of text data are CSV, or comma-separated values, that are intended to be used as tabular data; and JSON, or javascript object notation, which is used to convey data across the Web. But other types of text data also exist. HTML files are text-based, even though you see fancy page and text formatting, images and videos. Those items are rendered by your browser using instructions built into the text and separate non-text files held elsewhere on the site. Even modern versions of Excel files, with the .xlsx extension, are text based, though they are extremely complicated text that you wouldn’t want to deal with on your own. The most common non-text, or binary, files that you’ll encounter are PDFs – the form that most reports, forms and formatted documents are exchanged electronically – and some sort of image, sound or video file. Tabular data Not all text data is well suited to using as a data source. For example, we saw during the tidy data tutorial, that Excel files can be nearly useless for data analysis even though they’re conveyed through a spreadsheet. On the other end, files like JSON files are intended to be used for data analysis and presentation, even though they look difficult to deal with in the text editor. Common file formats CSV, TSV and other delimited data files Tabular text, the simplest data to read into any program, can come in several structures. The most common is called CSV, or comma-separated values. Your computer probably thinks that a CSV file is supposed to be opened in Excel or, on a Mac, in the built-in Numbers program. A CSV file suggests that a program wishing to use the data table should use commas to separate the columns. Usually, the data is enclosed by double quotes when it’s possible to have a comma within the field. The first row usually shows the headings in the same form: name, position, start_date, age_at_start_date &quot;Trump, Donald&quot;, President, 2017-01-20, 70 &quot;Obama, Barack&quot;, President, 2009-01-20, 47 If you see a file like this on the Web, you can open it in your browser and safely save it with a .csv extension so that other programs recognize it, but it’s not necessary. You can use it no matter what it’s called. Some people use TSV instead of CSV, replacing the comma with a TAB key. The reason is that it’s less likely you’ll need to use the quotes, which then reduces some confusion when your data could include quotes. They are much less likely to include TABS. Benefits CSV and its cousins are the international language of data. They work with every open source product and every piece of brand-name software. Traps Excel and Google sheets quite annoyingly like to think they know what’s in your data, and don’t let you specify the way the fields are read. This means that if it sees a number, it will treat it as a number even if it’s a Zip Code. It will also turn fields that look like dates into dates, even when they shouldn’t. This comes up frequently in two places. In anything related to schools that list the grades that school includes, you’ll often see ‘9-12’ turned into September 12th or September 2012. In some cities, addresses with dashes also get corrupted, turning 12-21 into December 12th. (This is very frequent in Queens, NY) These are very specific problems, but they are impossible to fix if you lose the original CSV file. TK - a video of how to do control Excel imports You can usually just change the extension from .csv to .txt and then you’ll be allowed to specify what you want the program to do. The other trap is harder to deal with and is treated differently in each program – that’s the problem of unescaped quotes and line endings in long fields. An unescaped quote would look something like this: Donald &quot;You&#39;re fired!&quot; Trump, president, ... In this case, programs won’t know what to do. Some, like Python, try to be smart – it works fine if there’s o comma directly after the internal quote. Others completely fail. Even worse, different programs expect different “escapes” for internal quotes. In MySQL and some other programs, a backslash tells the program to ignore the upcoming quote. Microsoft products usually want two consecutive ones: Donald &quot;You&#39;re fired! &quot; Trump Donald &quot;&quot;You&#39;re fired!&quot;&quot; Trump This is a common problem when someone has exported the data into a text file from some proprietary format like Excel. Fixed-width text This is another form of a universal file format that can be read by anyone, except it splits the fields at specific locations within a row rather than using specific characters. These are a little harder to read, but are common when you get data produced by old computers in government agencies. The bigger disadvantage is that every field has to be the same size. If someone has a long name or title, for example, it will be chopped off in this format. You will usually need a data dictionary or record layout to tell you how to split it apart. Trump, Donald president 2017-01-20 Obama, Barack president 2009-01-20 Biden, Joe vice president2009-01-20 Washington, Georpresident 1779-04-30 XML and JSON XML (eXtensible Markup Language) and JSON (Javascript Object Notation) are two other universal ways to share plain text data, but they work in a very different way from the tabular information we see. In both cases, they define a structure and can have multiple pieces of information within a record. They are both tagged, in that each field is named whenever it’s used. At its simplest, XML looks a lot like HTML (below), except it uses field names and characteristics to do its work: &lt;employees&gt; &lt;employee sex=&quot;M&quot;&gt; &lt;firstName&gt;John&lt;/firstName&gt; &lt;lastName&gt;Doe&lt;/lastName&gt; &lt;/employee&gt; &lt;employee sex=&quot;F&quot;&gt; &lt;firstName&gt;Anna&lt;/firstName&gt; &lt;lastName&gt;Smith&lt;/lastName&gt; &lt;/employee&gt; &lt;employee&gt; &lt;firstName&gt;Peter&lt;/firstName&gt; &lt;lastName&gt;Jones&lt;/lastName&gt; &lt;/employee&gt; &lt;/employees&gt;ß At one time, XML was the official format required by federal agencies for data transfer, so there are a lot of government-generated XML files. JSON also defines the structure of a dataset by using the names of the fields, but it’s stored in a somewhat more compact structure: {&quot;employees&quot;:[ { &quot;firstName&quot;:&quot;John&quot;, &quot;lastName&quot;:&quot;Doe&quot;, &quot;sex&quot;:&quot;M&quot;}, { &quot;firstName&quot;:&quot;Anna&quot;, &quot;lastName&quot;:&quot;Smith&quot;, &quot;sex&quot;: &quot;F&quot; }, { &quot;firstName&quot;:&quot;Peter&quot;, &quot;lastName&quot;:&quot;Jones&quot; } ]} Leaving out a field simply leaves it blank in the output dataset. Most Application Programming Interfaces (or APIs) return their data in .json format, which is easily converted into a complex object by javascript, Python or R. JSON was designed for efficiently transferring data to your browser, so it’s usually the most flexible format. Excel doesn’t open json by default, but Google sheets can easily import simple json objects. For both XML and json, look for online converters to .csv files online. They won’t handle large documents, but they’re perfectly good to convert files with a few hundred items in them. Benefits No one has to guess what anything means in these tagged structures. They can include anything you want, including full documents within one field. Once a computer program can read one of these formats, there are very few ways they can go wrong. Most computer languages such as R or Python have standard methods of reading these files, making them relatively easy to work with. Don’t curse these formats. They’re your friends. Traps They’re often NOT properly created, leading to frustrating “malformed” errors in programs that can read them. Complicated structures – ones with nested pieces of information, such as a list of nicknames for each of these employees – and very large files are almost impossible to open in software without reading it through a program first. HTML At its core, HTML is a structured document containing text. Images, sound, videos and other files are linked to them; the formatting you see is done through text instructions. That doesn’t mean it’s easy to work with as a data source. You’ll have to parse the document to get its pieces out. Traps Parsing HTML files has gotten easier, but it remains a time-consuming chore the first time you encounter the need. Most of the time, HTML is used when you scrape websites. Even when you do get the text out, there are often weird entities that are used for things like accents and other characters. For example, instead of a dash, “–”, you’ll see “&amp;mdash;” or instead of an “&amp;”, you see “&amp;amp;”. This is particularly annoying in lists of names with accents and other non-standard characters. Converting between the various text encodings can also be a headache, but it’s getting easier. (Old versions of Python choke on UTF-8 characters, like accents or smart quotes.) Proprietary file formats Most software saves files in special formats that they and only they can read. Some are so common, though, that there are converters and importers, or they have become de facto standards. Excel, Access and their cousins These are files the files created by software that contain special instructions in computer code that are intended to be read by the originating programs. Many of the formats you’ll see originated with Microsoft: .xlsx, .accdb, .docx and .pptx. In fact, Google docs downloads, by default, are converted into these Microsoft formats. Recent versions use a form of XML that contain detailed instructions on formatting, formulas, pages, and other Excel features. Don’t try to use them as text files. Every data system has a way to read Excel files. Some, like Google Sheets, reads them without any special instructions. Programming languages like R have importers. Benefits Excel files, when properly formed into tidy data, are among the easiest for sources to produce. They make PIO’s happy because they can be read by human beings using the programs that are on their desks already. They can also contain extra information, like a page of documentation or summaries and charts. Reading them into other programs is often seamless. Microsoft Access files are becoming less frequent, but it is a format that was used for many years by government agencies to transmit larger, more complicated, databases. They are much more likely to be “tidy” because they are designed to work with well formed data. Traps First, Microsoft doesn’t make a version of Access for Macs, which makes it more difficult to import – you’ll have to find a program or utility to convert them into something more universal. Excel can actually be a problem as well, which is why we advise reporters to request the data in a “plain text”, or CSV, format. If the data conveyed in the spreadsheet isn’t “tidy” it won’t do you much good. Text forms are much less likely to be presented as reports. One common issue is that a column contains different data types – other programs won’t know what to do with them. PDF I’ve saved PDF because it’s the bane of most data journalists’ existence. PDF files were designed to present information on a page exactly as the person who created it wanted. It’s really a report, not a dataset. But many government agencies now try to pawn off PDFs as data. There are all kinds of issues with PDFs, but for now just leave it this way: You don’t want them. This list suggests that, depending on the complexity of the data you are requesting, you’ll usually want one of the standard text-based formats: csv or tsv for simple data; xml or json for complicated data. Spatial data “Spatial data” is the technical term for data formats that come with information needed to create maps. The important thing to know is that whenever you are offered spatial data, or map files, take them. You can always extract tabular data from them, and they have a lot of other information that is usually very difficult to get. The most common file types are shapefiles (a bundle of files but .shp is the main extension), KML and GeoJSON, a special form of JSON files with instructions for drawing shapes. Traps The only problem with spatial data is that sometimes the person doesn’t give you what you need to know. My first mapping project went quite smoothly – as long as I put everything in Greenland instead of Maryland.",
    "url": "http://localhost:4000/cronkite-docs/general/file-formats.html",
    "relUrl": "/general/file-formats.html"
  },
  "17": {
    "id": "17",
    "title": "Public records",
    "content": "Your right to records",
    "url": "http://localhost:4000/cronkite-docs/records",
    "relUrl": "/records"
  },
  "18": {
    "id": "18",
    "title": "1-day CAR workshop",
    "content": "Fundamentals of data reporting Columbia University, March 2015 Schedule: 10:00 Introductions and expectations for the day 10:45 Data journalism in the wild 11:15 Excel review: best practices and tidy data 11:45 First exercise: an internal affairs report 12:30 Lunch and review of morning. Optional discussion of acquiring data using open records laws. 1:30 More in Excel: Pivot tables, filtering and how to extend to other programs 2:30 Tableau Public: Sneaking into SQL with visualization 3:30 Getting data: The basics of web scraping and APIs Stories in the wild Elliot Jaspin’s story of finding racial cleansing in Census data. Money as a Weapon (started with database), The Washington Post, Dana Hedgpeth and Sarah Cohen Jo Becker and Ron Nixon’s Iran story (good example of “what’s supposed to happen?”), The New York Times Medicare, The Wall Street Journal, (very, very large fish) 60 data-driven ideas in 60 minutes from Mary Jo Webster and Jodi Upton, NICAR 14. A Death in St. Augustine, Frontline, as an example of weaving in data when you are working on a narrative. We’ll look at a piece of this story later on. The presentation from our class Mike Berens’ piece on the heart of data journalism, written when Bill Clinton was president. Excel: Review and best practices Handouts and reference Cheat sheet on keyboard shortcuts for Mac and Windows Excel. Video series on best practices (for refresher later on) In class: We’ll look through this reconstruction of the career of Dr. Jack Kevorkian to see how to build a chronology for what we need. Create the outline of a spreadsheet on Bubba Harris based on the internal affairs report (not linked). You’ll decide which columns you need and how you’ll want to use it in a story. Try filling in about 5-10 rows. After class, you can see the spreadsheet I created from it, and the paragraphs in a story that were based on it (toward the end). If you’re interested in other data work for that story, hthe companion piece is []”Departments Slow To Police Their Own Abusers”](http://www.nytimes.com/projects/2013/police-domestic-abuse/) Excel Part 2: Filtering and pivot tables Handouts and reference Videos on filtering and pivot tables Handout, courtesy of IRE. In class Working with Major League Baseball salaries If there is time: Open refine to clean data A start-to-finish example of using OpenRefine to create a tidy dataset, which we’ll use in the next step, using long-term managed care populations from the NYS Department of Health. Original spreadsheets downloaded from this state site and combined to create a single spreadsheet with all reports on one page. We’ll start from here, with this guide for OpenRefine. Database concepts and viz: Tableau Public Training materials for IRE conferences. (Note: If you join IRE for $70, you can request a copy of desktop Tableau for free - normally about $2,000 / year.) Example dat Going further Public records resources from previous class",
    "url": "http://localhost:4000/cronkite-docs/workshops/fundamental1day.html",
    "relUrl": "/workshops/fundamental1day.html"
  },
  "19": {
    "id": "19",
    "title": "General resources",
    "content": "These are tutorials and resources that don’t depend on any particular method or technology. Some are just links to resources, others are longer tipsheets or tutorials. Here are some presentations I’ve done that fit into the “General” category: The Data Journalism Ecosystem, Nov. 2017, for the Cronkite School first-year journalism students. Empirical Journalism, a regularly updated survey of data-driven reporting. Investigative techniques, a more detailed deconstruction of a few stories to show how to find and use records in hard-to-quantify stories.",
    "url": "http://localhost:4000/cronkite-docs/general",
    "relUrl": "/general"
  },
  "20": {
    "id": "20",
    "title": "Home",
    "content": "Most of the materials for the Cronkite School’s data journalism course taught by Knight Chair Sarah Cohen are kept here. It’s also a holding center for some other presentations, like FOIA workshops. This site is replacing a lot of the other repos I’ve made over the years to collect data reporting and public records materials. I’m trying to get more organized. It’s also an attempt to rein in dead links and outdated materials that no longer work. The sections are: General resources A collection of data reporting resources, including re-introductions to math and stats, some best practices and links to stories and projects that can exemplify the data reporting genre, especially in investigative and enterprise journalism. There is also a series of links on background in data journalism. Excel Tutorials and exercises using Excel for daily chores. These are conceptual as well as technical – you’ll find some of those concepts in repeated in other sections. R stats Some resources and markdown files for learning basic R to replace some of your Excel workflow. Specialized tools Anything that is used for one purpose, such as OpenRefine for data cleaning, or the Chrome Scraper for scraping. Public records Resources to understand and acquire public records using U.S. federal and state laws. It also includes backgrounding tips and tricks. Workshops Where I stash my presentations for various workshops. Some are links to other repos",
    "url": "http://localhost:4000/cronkite-docs/",
    "relUrl": "/"
  },
  "21": {
    "id": "21",
    "title": "Background on Misplaced Trust #",
    "content": "Background on Misplaced Trust Evolved out of an anecdotal story from a beat, by Carol Leonnig. Often the source of the best stories. Court refused to provide records. We eventually got them, but could not have done the story without them Records did not exist in a way that would allow for the analysis we wanted to do, so we cleaned them up and matched them against other records to get the data. Different from anecdote because we knew our examples were typical, regardless of what the court said it did. No one else had ever done this story, at least in this way. Several years later, the LA Times won a lot of awards for a story very much like this one. It’s a pretty classic, and straightforward, example of computer-assisted reporting. It’s not the biggest story ever, nor the most complex, nor the most groundbreaking in methods. But it shows what can be done just with a couple of data sets and some elbow grease: use original research to hold a system up to scrutiny, and examine its performance against its own standards. In this case, they were judges’ own orders and local law. (Len Downie, 1963??, also a project on DC probate court.) Virgnina Hinton Meet Virginia Hinton. The best description of her came from the story: Virginia Hinton was a Radcliffe graduate, world traveler and the former wife of a history scholar. But in her most vulnerable years, her life came undone. She suffered first from alcoholism, then mild dementia. At 75, she owned a house in Rockville but ended up living on street corners. Ms Hinton was left on the streets by the system that had promised to protect her. A court-appointed guardian told judges that she “preferred to live on the streets,” but that she also complained that her furniture was urine-soaked when Ms. Hinton would come by each month for her living money. Where that money went no one knows. The lawyer, Rozan Cater, was found to have misplaced at least $250,000 from clients’ accounts. In Hinton’s case, she failed to use the money to pay property taxes. She let it go so long that the county confiscated her property. Ms. Hinton one of the people we found in a corner of the courthouse – one better known for wills and estate battles – called Probate Court (Surrogate’s Court in New York State). Judges there decide whether the old and infirm are vulnerable enough – and alone enough – to warrant assigning a guardian or conservator to protect their interests. The Washington Post’s story, like most others on this court around the country, found that once that guardian was assigned, no one bothered to check on how they were doing despite laws requiring the court to oversee the care. Our story focused on the lawyers who were on a voluntary list to be assigned to this task. They get paid very little when the ward is poor, but can get a windfall by managing wealthy clients’ cash. Bottom-up reporting This was an example of bottom-up reporting. We looked at the docket for the earlier case, and tried to find the M.O. We had to go through several more examples before we knew what would be required to get it right. I use the term to mean that you start with one example or tip, dip back into the data to see if you can find another, go back to the field and check it out. Rinse, repeat. Keep going between field and lab, field and lab until you’ve assured yourself you know what can go wrong with your analysis and what it will take to get it right. You might stop here – we often do – with several very good cases and practices that illustrate problems when you realize you can’t go further. Examples finding examples and moving on: Part One of the story you’ll read later on on the Bronx Courts. The later story was driven by bottom-up data work. Recent story in the Times on military hospitals, which was inspired by entries in a database, but reported in a more traditional way. Upcoming story on Rikers, I think. I can’t tell you much about it now, but you’ll know it when you see it. Bottom-up often has a phrase in it about “the most” or “one of the most” – it alerts you to an outlier. Top-down reporting Sometimes, when you can’t get individual records, you have to try to drill down from statistics. It’s often more explanatory than investigative; often geographic. It depends on you knowing what you are looking for, and how that trend would look in the data. It also means you can’t be sure how your example looks in the original data. It’s a fall-back. Getting the data The analysis portion of this story began with serendipity. Two reporters, then working on a project about a woman who had been unfairly placed into a conservatorship and held hostage in a DC nursing home by a judge, visited the chief judge of our Superior Court. (She happened to be the Commerce Dept. official who invented the formula used to determine whether families lived in poverty.) During the reporting on that case and one earlier one, the court had removed from the probate division the only computer that allowed us to look up probate cases. The just got rid of it. Carol and Sari Horwitz, who I’d worked with on two other projects, asked me to come to see if we could get Judge King to give us access again. At that meeting, in June 2002, King said he would make a terminal available in the depths of the building, and directed his IT staff to work it out with us. He refused to turn over a copy of the docket electronically, saying he preferred to keep it in the building. Meanwhile, Sari moved to a different story and Lena Sun joined the project. We went over there and started looking up cases. It turns out that their system didn’t allow us to find all of the cases of any lawyer, so we were going to have to look at them all just to get the history on a particular attorney., We’d just have to go case by case, screen by screen and download each. A guy named Ron was assigned to mind us. Lena brought cookies, one of us showed up every day and just sat there downloading the cases onto a floppy disk. We’d spent a week or so and downloaded what we needed for just a handful of cases. When they asked how long we’d need access to their systems and the building, we said it could take a year. We promised to keep bringing cookies. Then they asked what it would take to get rid of us. We said, if they would just give us the database, we’d be gone the next day. They did. We had it on tape a few days later, in July of 2002. I don’t think Judge King ever realized we got the full docket. He never asked how we did this. Now came the hard part – trying to decipher the codes in the dockets to see if we could identify lawyers and to find cases in which a guardian had been punished for neglecting his ward. Virginia Hinton docket## Docket headers The first thing to notice is that the named people in the case, Ken Loewinger and Stephanie Grogan, among others, are not Rozan Cater – the lawyer who left her on the streets. In fact, there is no mention of Cater until you get way lower in the docket. That’s why their in-house search wouldn’t work. But there are a few hints here that something has gone awry. The first is the code next to Loewinger’s name that says “SC” and “SG” – Ron had helpfully given us a list of about 300 codes that are inconsistently used in the dockets and their meanings. These ones mean “Successor conservator” and “Successor guardian”. It means this isn’t the first person to have custody of the ward. Either the previous guardian resigned, died, or was stripped of her powers. There’s another hint on this record that it might have victim: An estimated value of the estate at $320,000. Section 2### Here is the next section of this docket: It is where cater’s name first comes up – 2 ½ years after first being appointed to care for Hinton’s health and safety and money, she filed her first accounting of the funds. These are supposed to be filed every year. Let me walk through with you some interesting pieces of this docket: – After being reminded by the court, Cater filed a “Guardianship report” in December 1996– only about 3 months after it was first due, and after being reminded. But then 2 ½ years go by before the court notices that anything was wrong and sent a delinquency notice (which was ignored) and then scheduled what’s called a “show cause” hearing for “failure to file”. The delinquencies continued until someone in the family – Page Stodder – noticed. Virginia has now been on the streets for at least 4 years. She has at least temporarily lost her home because Cater had failed to pay property taxes (the county holds it for a time before selling it at a tax sale so she did eventually get it back.) Section 3 Within a few months, Cater had been removed from the case and less than a year later she was referred to the lawyer’s disciplinary committee for failure to account for the missing funds. But she was never punished by the judge: She was allowed to resign rather than be fired from the case, was left on the list of attorneys eligible for new appointments, and was only referred to bar counsel after Loewinger did a full accounting – something that, save Page Stodder’s discovery of her aunt’s condition would normally never happen. The other problem I just want to note for the data nerds in the room is to notice the code next to the “referred to bar counsel” . That is supposed to be real code – ORBC, for Order to Refer to Bar Counsel. It wasn’t used, but instead was “TT00” indicating a free-text field. What this case showed us was what to look for in a case that might have these problems. So I did some searches that created specific lists that we could go check out – any mention of a bar counsel, and mention of “granting leave to resign” or, worse, “order of removal”; These lists gave us the fodder to go back to the original records and the street to check out what happened in other cases. ##Gloria Johnson## Gloria Johnson was another lawyer who repeatedly received appointments from judges even though she had failed to do basic work, from showing up at hearings to filing any reports on the status of her wards. She was repeatedly punished by judges, but then just as quickly appointed to new jobs. In all, the Post found that judges had appointed lawyers 240 times over about 10 years AFTER they had imposed sanctions for failing to do their jobs. In a small system, as DC’s in – and in cases in which the only time sanctions were imposed was when someone complained – this was a staggering number. ##Analysis of oversight## One of the analyses we conducted was to find out how often basic laws were followed in the care of the wards. This posed a particular problem: By definition, these wards were alone with no responsible family members or close friends to oversee their care. Many were unable to coherently talk themselves, and had limited credibility. Moreover, many died within months of being put under a guardian’s care. So we could not evaluate how many were neglected, abused or otherwise victimized. All we could do was ask, “is anyone watching?” and the resounding answer was, “NO”. By law, the only oversight of guardians comes from the Probate court in DC. And by law, every guardian must turn in a form every six months attesting to the visits he made to the ward, changes in their health and any differences in their living conditions. From court dockets, we were able to find 783 cases in which a guardian was assigned for at least a year (most of the rest died before a year was out.) In nearly half, the court had not heard back from the guardian in over 18 months. In about a sixth of the cases, the court NEVER received a report. And the court did little to prod them or inquire about the resident’s health or well-being. The other side of the coin While we were looking at this problem, we started seeing the other side of the problem: people shoved into guardianship who didn’t feel like they needed it, or who had no meaningful review of the request by the judges. No data? Get your own There were certain steps required by law to have a guardian appointed, and we suspected they were being sidestepped. But the only record of what happened was in the paper records in the courthouse. We arranged for four of us to go to the courthouse one day and review every case filed over six months. (As an aside, we found our dockets were far more accurate than the paper documents, which were often misfiled or otherwise inaccurately identifying court dates and other key pieces of information. The judge relies on the docket. No one really looks at the case file.) ##Lessons for investigative reporting## What is supposed to happen? Compared to what? In these cases, we had easy comparisons: they were right in the judges’ orders and in the local law. Find one case you know about, look for the M.O. Virginia Hinton was our case. It led us toward the codes and phrases in the docket we should seek for other examples. Focus on time, place and people Matching lists – eligible lawyers, dockets. We had to distinguish family members from lawyers, so used the eligibility list to isolate them. Often requires programming - you would have a hard time looking for how long between reports if you had to do this using Excel or an SQL product. What you start doing is rarely what you end up doing. We started with a single conservatorship case, ended up focusing on guardians. Reason: There were many more, and these were the people who needed the help the most. Look at both sides of a coin: people neglected in the system; people in the system who shouldn’t be. Reaction This was the strangest reaction I’ve ever seen. We spent at least 4 hours with Judge King going over our results with him. He just didn’t engage. No matter how many times we told him the problems were rampant, he saw them as cases that fell through the cracks. He called the day the story ran and said he just didn’t get it until he saw it on the front page of the paper, all laid out with data and stories. It was a powerful reminder that people can know something is true, but not feel it, until we do our jobs.",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/lecturenotes-guardians.html",
    "relUrl": "/assets/docs/lecturenotes-guardians.html"
  },
  "22": {
    "id": "22",
    "title": "Newsroom math",
    "content": "Numbers in the newsroom Statistics are people with the tears washed off - Paul Brodeur Jo Craven McGinty, then of The New York Times, used simple rates and ratios to discover that a 6-story brick New Jersey hospital was the most expensive in the nation. In 2012, Bayonne Medical Center “charged the highest amounts in the country for nearly one-quarter of the most common hospital treatments,” the Times story said. To do this story, McGinty only needed to know the volume of the procedures reported by the government and the total amount each hospital charged. Dividing those to find an average price, then ranking the most common procedures, led to this surprising result. Why numbers? Overcoming your fear of math Put math in its place Reading and viewing How high school algebra won a Pulitzer Prize Exercises Why numbers? Using averages, percentages and percent change is the bread and butter of data journalism, leading to stories ranging from home price comparisons to school reports and crime trends. It may have been charming at one time for reporters to announce that they didn’t “do” math, but no longer. Instead, it is now an announcement that the reporter can only do some of the job. You will never be able to tackle complicated, in-depth stories without reviewing basic math. The good news is that most of the math and statistics you need in a newsroom isn’t nearly as difficult as high school algebra. You learned it somewhere around the 4th grade. You then had a decade to forget it before deciding you didn’t like math. But mastering this most basic arithmetic again is a requirement in the modern age. In working with typical newsroom math, you will need to learn how to: Overcome your fear of numbers Integrate numbers into your reporting Calculate change in a list of items Use different types of averages as summaries of a list of items. This tutorial has some general tips for dealing with numbers and resources for going further. The common arithmetic you need is covered in the Excel module on formulas. Overcoming your fear of math When we learned to read, we got used to the idea that 26 letters in American English could be assembled into units that we understand without thinking – words, sentences, paragraphs and books. We never got the same comfort level with 10 digits, and neither did our audience. Think of your own reaction to seeing a page of words. Now imagine it as a page of numbers. Seasick yet? Instead, picture the number “five”. It’s easy. It might be fingers or toe or it might be one team on a basketball court. But it’s simple to understand. Now picture the number 275 million. It’s hard. Unfortunately, 275 billion isn’t much harder, even though it’s magnitudes larger. (Remember this: 1 million seconds goes by in about 11 days. 1 billion seconds goes by longer than you’ve probably been alive, or more than 35 years.) So the easiest way to get used to some numbers is to learn ways to cut them down to size, by calculating rates, ratios, percentages and rounding off. This video will give you a more in-depth pep talk on getting used to numbers. Put math in its place For journalists, numbers – or facts – make up the third leg of a stool supported by human stories or anecdotes , and smart or insightful insight from sources. They serve us in three ways: As summaries. Almost by definition, a number counts something, averages something, or otherwise summarizes something. Sometimes, it does a good job, as in the average height of Americans. Sometimes it does a terrible job, as in the average income of Americans. Try to find summaries that accurately characterize the real world. As opinions. Sometimes it’s an opinion derived after years of impartial study. Sometimes it’s an opinion tinged with partisan or selective choices of facts. Use them accordingly. As guesses. Sometimes it’s a good guess, sometimes it’s an off-the-cuff guess. And sometimes it’s a hopeful guess. But even the most accurate of numbers can be guesses, such as the 2017 election in Virginia that swung the state legislature by a single vote in Newport News. If there are slight differences between numbers, maybe they’re not worth writing about. Once you find the humanity in your numbers, by cutting them down to size and relegating them to their proper role, you’ll find yourself less fearful. You’ll be able to characterize what you’ve learned rather than numb your readers with every number in your notebook. You may even find that finding facts on your own is fun. Reading and viewing Listen to the lecture on YouTube on overcoming your fear of numbers. Read “Numbers in the Newsroom,” a $10 beat book from Investigative Reporters and Editors. Concentrate on the first two chapters. Matt Waite’s math review “Avoiding Numeric Novcain: Writing Well with Numbers,” by Chip Scanlan, Poynter.com How high school algebra won a Pulitzer Prize If you were at all paying attention in pre-college science classes, you have probably seen this equation: d = rt or distance = rate*time In English, that says we can know how far something has travelled if we know how fast it’s going and for how long. If we multiply the rate by the time, we’ll get the distance. If you remember just a bit about algebra, you know we can move these things around. If we know two of them, we can figure out the third. So, for instance, if we know the distance and we know the time, we can use algebra to divide the distance by the time to get the rate. d/t = r or distance/time = rate In 2012, the South Florida Sun Sentinel found a story in this formula. People were dying on South Florida tollways in terrible car accidents. What made these different from other car fatal car accidents that happen every day in the US? Police officers driving way too fast were causing them. But do police regularly speed on tollways or were there just a few random and fatal exceptions? Thanks to Florida’s public records laws, the Sun Sentinel got records from the toll transponders in police cars in south Florida. The transponders recorded when a car went through a given place. And then it would do it again. And again. Given that those places are fixed – they’re toll plazas – and they had the time it took to go from one toll plaza to another, they had the distance and the time. It took high school algebra to find how fast police officers were driving. And the results were shocking. Twenty percent of police officers had exceeded 90 miles per hour on toll roads. In a 13-month period, officers drove between 90 and 110 mph more than 5,000 times. And these were just instances found on toll roads. Not all roads have tolls. The story was a stunning find, and the newspaper documented case after case of police officers violating the law and escaping punishment. And, in 2013, they won the Pulitzer Prize for Public Service. All with simple high school algebra. Exercises In class: Create a spreadsheet with basic numeric information about your school, such as enrollment this year and last; graduation rates; and tuition. Use basic math to show change, percent change and averages. Imagine it is Jan. 1, 2018 and you are tasked with writing the annual weather story, summarizing the high and low points of the previous year. Using this daily summary of temperatures, rain and wind for Phoenix, try to find three interesting facts for your story. If you want to download your own data from NOAA, choose “Local Climatalogical Data,” and keep only the rows that refer to “SOD,” or “Summary of Day”. Download the population estimates for the nation and find the fastest-growing and fastest-shrinking counties in the nation over past five years. – Sarah Cohen and Matt Waite, July 2018",
    "url": "http://localhost:4000/cronkite-docs/general/newsroom-math.html",
    "relUrl": "/general/newsroom-math.html"
  },
  "23": {
    "id": "23",
    "title": "Backgrounding resources",
    "content": "Backgrounding resources IRE Tip Sheet #2529, “Backgrounding People,” Pat Stith, Raleigh News &amp; Observer, 2005 IRE Tip Sheet #3358, “Backgrounding People and Businesses,” Jaimi Dowdell, 2010 IRE Tip Sheet #4177, Another backgrounding handout from IRE and its companion, The Omaha World Herald&#39;s quest for the background of a crime family&lt;/a&gt; IRE Tip Sheet #3238, “Ways to Trace Every Business,” Ron Campbell, 2010 IRE Tip Sheet #3716, &quot;Uncovering the private parts of public figures&quot;, Doug Haddix, 2012 Tip sheet from the Stabile reporting program &lt;/ul&gt; ### One approach Ask these questions about any person, company or property: * What do they say about themselves? * What do others say about them? * What&#39;s missing from both accounts? * What has been mischaracterized (on purpose or by accident)? New York resources SeeThroughNY, a public interest group with New York city payroll, pension and contract information obtained through public records The New York City Finance Department site contains property records, including mortgage and deed information. You will probably have to look up the block and lot information using the address lookup to get much information. I&#39;ve had trouble with the latest Java problems getting the actual documents online. The New York State campaign finance office has searchable contributions Many records require you go to the office to request information: New York voter information is only available online if you know the full name and date of birth of the person you&#39;re looking for. New York State e-courts include varying levels of access to criminal, civil, housing, family and other courts. Don&#39;t forget about federal records: campaign finance through the Center for Responsive Politics, court cases and bankruptcies through PACER (a pay service that is very inexpensive), and business records from the Occupational Safety and Health Administration, the Small Business Administration and the Environmental Protection Administration. Many of these records are collected by Enigma.io (free but you you have to sign up)",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/foia/nyc-backgrounding.html",
    "relUrl": "/assets/docs/foia/nyc-backgrounding.html"
  },
  "24": {
    "id": "24",
    "title": "PDF Tools",
    "content": "PDF Tools PDF’s are the bane of a data journalist’s existence. They come in all kinds of unfriendly formats. This list of tools is just for those PDFs you want to turn into a data table of some kind, not ones that you just need to search as documents. #Searchable tables Some PDFs were created from an Access or Excel printout, and have within them the original text. Others have been effectively scanned and are just images. You can tell the difference by trying to select some text. If you can select text then you have the first step done. The problem is that PDFs are designed to preserve the format on the page, and aren’t data files. They’re like printed reports. Sometimes copying and pasting leaves you with unintelligible line breaks and smushed-together cells. You’ll usually have to use some kind of tool to deal with this. The key to trying to convert PDFs is to try every tool you can afford. You’ll never know which one works. Free tools CometDocs is a free service that lets you upload your pdf and will return to you an XL file in email. Your IRE membership gives you a paid version for free, which lets you keep all of your pdf’s on their site and puts you in the front of the line. Tabula, a new alpha-level project from ProPublica and the Mozilla fellows (and also Jeremy Merrill now at the NYT). It lets you show the program where a table is, and it does a little more sophisticated work to figure out where the cells are. It’s a little better with PDF’s with lines between rows than CometDocs. xpdf is an old but reliable command line tool that still works remarkably well. It’s often best for very fixed-width and consistent pdfs. It’s a bit of a pain to install on Unix-based (including Mac) systems, but once you have it it’s easy. See this tutorial from IRE to use it. You’ll have to log into IRE to get it. There are also PDF plug-ins to Ruby and Python, which can sometimes work just fine. None of these tools will convert images to words – for that, you may need to pay some money. ##When free isn’t good enough Most of the pay products have two levels: one if you don’t need “OCR”, or Optical Character Recognition, for scanned documents; another if you do. If you’re going to pay, you might as well get the OCR – you’ll need it someday. In order of difficulty and cost: *Able2Extract: $99.95 for non-OCR/ $129.95 for OCR. Intuitive and reasonably accurate. It has some annoyances, but saving as a text file often overcomes them (rather than going directly to Excel). *ABBYY FineReader Also $99.99 for personal use. Rob had a great tutorial on this. In terms of speed, ease of use and accuracy, it’s probably the best investment. It will also work very well on documents that have been scanned to turn them into searchable PDFs. *OmniPage, $149.99 for individuals. This one is just slightly more powerful, but I’ve had better luck with it over time. The constant upgrades are really a pain. *Cogniview has PDF2XL, which has been a treat for very difficult pdfs. It has a hard learning curve and is only really useful on tables, unlike OmniPage and ABBYY. ($129.97). Some of these come for both Mac and Windows, others just for Windows.",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/pdf_tools.html",
    "relUrl": "/assets/docs/pdf_tools.html"
  },
  "25": {
    "id": "25",
    "title": "Links to source data for doctoral students",
    "content": "Outside data sources for doctoral students Social Explorer is a much easier way to get at Census and related data than going through the Census bureau. It’s a subscription service that is available any time you’re connected through the school’s IP. It includes block group level 5-year ACS files, decennial census and some historical data other sources. IPUMS is a project out of the University of Minnesota that collects microdata and tries to line it up across years. This means you can compare Census results for some geographic areas over a very long time. Most of the time, you can generate your results online, which takes care of weighting and error testing. In some cases, you’ll have to download the data and create summaries yourself. Of particular note is the Time-Use survey and the CPS voter supplement. General Social Survey has been taken every other year since 1972, and include a varying set of questions on social issues. It’s a difficult site to use, and trend data is shown online only for some predetermined issues. Be careful with the weighting - in two years, they oversampled African-American respondents, which means you have to remove that sample code to make years comparable. Using wtsall as the weight will take care of these problems. ICPSR, or the Inter-university Consortium for Political and Social Research, lets researchers and government agencies archive their raw data. Almost all data collected by and for the Bureau of Justice Statsitics of DOJ is archived here. What does exist on this site is well documented. Most datasets are available in SPSS, SAS or other statistical formats. More recently, they are supplying R code or packages to read the data. Roper Center for Public Opinion Research is an archive of reputable polling results. Most organizations contribute them to Roper after some time has passed, but it’s a useful place to see what questions on your topic have been asked before, and by whom. One way to use it is to do a new survey based on the same questions if you need a comparison. Search poll questions using the ipoll search. Their datasets are mainly historical or esoteric. enigma.com wants to be the Google for public records, and it has aggressively sought out some datasets from government agencies. (Example: The minority report on Facebook ads allows you to look at the data submitted to Congress during the investigation) One of its great strengths is that it takes care to give you the lineage of all datasets. data.world wants to be the Facebook of data, where people can share their data and insights. It’s all over the place – no one makes any effort to curate it, or to enforce credibility standards. But if you find something there, you can usually follow it back to the original source. github.com is where a lot of people stash their data, and by default it’s public. A Google search usually doesn’t search the datasets held inside Github’s world - you normally need to search there. It’s also useful to look for news organizations’ github pages – this is where they make data public that they’ve used in stories. (There are some other places, but this is the most common.) Google’s new dataset search makes it a little easier to find standard datasets, but it is just as hit or miss as the rest of Google.",
    "url": "http://localhost:4000/cronkite-docs/workshops/phd-sources.html",
    "relUrl": "/workshops/phd-sources.html"
  },
  "26": {
    "id": "26",
    "title": "Outside in reporting",
    "content": "Many reporters approach a long-term project by researching “from the outside in”. This means circling around a topic using publicly available information and people on the fringes of the topic before they approach a key source. In our case, your goal is to know exactly what you are looking for and what your rights are before you ever contact the agency. I will provide you some examples of public records requests that were developed that way, and there is an example in your textbook. Generally, here is what you need to learn before you request electronic public records: The specific agency that holds the records you want to acquire. The formal name of the form or database that you want. Which portions of the records are public under the relevant law. Taming your topic Start by thinking of a story idea or a line of coverage that would benefit from some public records. These aren’t just ideas that you think need some “numbers” – if that’s all you want, then a special interest group or expert can probably just give them to you. Instead, think of something that requires individual levels of information or a story that isn’t just dependent on one or two facts. Example: You want to report on wildfires in Arizona. If you already have a story, you may just want to know how many lives, acres and dollars have been destroyed by wildfires in recent years. This fact is readily available from a variety of sources. But if you want to delve into the details of each fire, including exactly where it started, how it started and how it spread, you are likely to need more granular data: records on each fire. Get inspired You should do a thorough search on where you are on this story. Who else has done something similar? Was it a long time ago, or in a different area? What did other local coverage miss? Sources to check include: IRE resource center, story database and story packs. To find stories, be sure to log into IRE using your membership account, and search in the main page search box, not the sidebar: Most large non-profit news investigations of your topic will have an IRE entry. Once you find it and are logged in, you can download the contest entry that shows what sources they used and what problems they had reporting the story. Do a targeted Nexis or Proquest search through the library, limiting your results to long stories that contain keywords that will limit your results: Do a good Google News and Google search. For example: news investigation allintitle: wildfires In each of these sources, pay attention not only to what they’ve found, but to the experts they quote, the organizations they mention and the documents or data they source in graphics and in the stories. Find people who know more than you Look for: Congressional (or state level) legislative testimony Interest groups that cover your topic. You can use the Encyclopedia of Associations through the library, or look at Project VoteSmart’s special interest group list for national organizations. Then look at their websites to see if this is a topic they care about. People who have been involved in the issue. Retired agency employees - especially inspectors - are usually quite knowledgable and are interested in helping the public understand what they used to do. You might also look for lawyers who represent people hurt by the system you want to examine, or victims themselves. Academic researchers who are invested in your topic. Use Google Scholar to find them. Be sure to read through any relevant articles they have published before you call them. Back to IRE, this time for the Tip Sheets section. (search it the same way you do stories, using the box in the center, not ) In each of these sources, you should focus not on their findings or opinions, but on HOW they do their work, and what documents or data they rely on. Find other data that has been released Look on websites of other cities, states and the federal government to find what data might have already been released. Don’t expect it to be complete, but it will give you a least common denominator for a lot of sets of records. Federal records are often a subset of what’s collected at the state level, which is in turn a subset of what’s collected by counties or cities. As an example, water quality measures are taken one by one by individual water utilities sampling specific households. Those records are often on paper. They are typed into a database, then usually forwarded to a state agency, which standardizes the format at sends them along to the federal EPA. At every stage, some details are lost. Some places to look: Public records aggregators : data.world, enigma.io’s public site, muckrock.org. There may be data aggregators for your specific topic as well. Open records portals in the federal government and some other states and cities. Other places that have released data in the past include California (state), Oakland, New York State, New York City, Washington DC and Chicago. Think of these as the minimum you should be able to get in most areas. Narrow your topic and your story ideas Now that you have a little background and can guess how difficult it will be to get records, you’re ready to narrow your story ideas and your plan for acquiring the records. For our purposes, remember that the records cannot be readily available on the Internet, and should come from state or local government agencies in Arizona. Burrowing in on your agencies Now it’s time to figure out what might be available to you from your own state or local government agency. This involves learning the specific name of the document set or database, and which agency is responsible for it. You’ll also have to research the law to understand what can and cannot be released. Before you request records, you have to understand: What agency is responsible for them – is it the state or a county? Which department? Which part of the department? You’ll do that by browsing their website, looking at their budgets and searching for any audits or inspector general reports that describe the process. You may also want to call the legislative staff of the committees in the State Legislature that oversee the program that collects the records. What is and isn’t public from those records? Don’t just take what they say – instead, study the law and determine if you have a right to some portion of the records. NOW you can call the agency and ask how you go about getting the records – you should ask for them by name, and make sure you are talking to the right person.",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/public-records-outside-in.html",
    "relUrl": "/assets/docs/public-records-outside-in.html"
  },
  "27": {
    "id": "27",
    "title": "Public records strategey",
    "content": "(TK: UPDATE LINKS) Reporting from the outside in Many reporters approach a long-term project by researching “from the outside in”. This means circling around a topic using publicly available information and people on the fringes of the topic before they approach a key source. In our case, your goal is to know exactly what you are looking for and what your rights are before you ever contact the agency. I will provide you some examples of public records requests that were developed that way, and there is an example in your textbook. Generally, here is what you need to learn before you request electronic public records: The specific agency that holds the records you want to acquire. The formal name of the form or database that you want. Which portions of the records are public under the relevant law. Taming your topic Get inspired Find people who know more than you Find other data that has been released Narrow your topic and your story ideas Burrowing in on your agencies Taming your topic Start by thinking of a story idea or a line of coverage that would benefit from some public records. These aren’t just ideas that you think need some “numbers” – if that’s all you want, then a special interest group or expert can probably just give them to you. Instead, think of something that requires individual levels of information or a story that isn’t just dependent on one or two facts. Example: You want to report on wildfires in Arizona. If you already have a story, you may just want to know how many lives, acres and dollars have been destroyed by wildfires in recent years. This fact is readily available from a variety of sources. But if you want to delve into the details of each fire, including exactly where it started, how it started and how it spread, you are likely to need more granular data: records on each fire. Get inspired You should do a thorough search on where you are on this story. Who else has done something similar? Was it a long time ago, or in a different area? What did other local coverage miss? Sources to check include: IRE resource center, story database and story packs. To find stories, be sure to log into IRE using your membership account, and search in the main page search box, not the sidebar: Most large non-profit news investigations of your topic will have an IRE entry. Once you find it and are logged in, you can download the contest entry that shows what sources they used and what problems they had reporting the story. Do a targeted Nexis or Proquest search through the library, limiting your results to long stories that contain keywords that will limit your results: Do a good Google News and Google search. For example: news investigation allintitle: wildfires In each of these sources, pay attention not only to what they’ve found, but to the experts they quote, the organizations they mention and the documents or data they source in graphics and in the stories. Find people who know more than you Look for: Congressional (or state level) legislative testimony Interest groups that cover your topic. You can use the Encyclopedia of Associations through the library, or look at Project VoteSmart’s special interest group list for national organizations. Then look at their websites to see if this is a topic they care about. People who have been involved in the issue. Retired agency employees - especially inspectors - are usually quite knowledgable and are interested in helping the public understand what they used to do. You might also look for lawyers who represent people hurt by the system you want to examine, or victims themselves. Academic researchers who are invested in your topic. Use Google Scholar to find them. Be sure to read through any relevant articles they have published before you call them. Back to IRE, this time for the Tip Sheets section. (search it the same way you do stories, using the box in the center, not ) In each of these sources, you should focus not on their findings or opinions, but on HOW they do their work, and what documents or data they rely on. Find other data that has been released Look on websites of other cities, states and the federal government to find what data might have already been released. Don’t expect it to be complete, but it will give you a least common denominator for a lot of sets of records. Federal records are often a subset of what’s collected at the state level, which is in turn a subset of what’s collected by counties or cities. As an example, water quality measures are taken one by one by individual water utilities sampling specific households. Those records are often on paper. They are typed into a database, then usually forwarded to a state agency, which standardizes the format at sends them along to the federal EPA. At every stage, some details are lost. Some places to look: Public records aggregators : data.world, enigma.io’s public site, muckrock.org. There may be data aggregators for your specific topic as well. Open records portals in the federal government and some other states and cities. Other places that have released data in the past include California (state), Oakland, New York State, New York City, Washington DC and Chicago. Think of these as the minimum you should be able to get in most areas. Narrow your topic and your story ideas Now that you have a little background and can guess how difficult it will be to get records, you’re ready to narrow your story ideas and your plan for acquiring the records. For our purposes, remember that the records cannot be readily available on the Internet, and should come from state or local government agencies in Arizona. Burrowing in on your agencies Now it’s time to figure out what might be available to you from your own state or local government agency. This involves learning the specific name of the document set or database, and which agency is responsible for it. You’ll also have to research the law to understand what can and cannot be released. Before you request records, you have to understand: What agency is responsible for them – is it the state or a county? Which department? Which part of the department? You’ll do that by browsing their website, looking at their budgets and searching for any audits or inspector general reports that describe the process. You may also want to call the legislative staff of the committees in the State Legislature that oversee the program that collects the records. What is and isn’t public from those records? Don’t just take what they say – instead, study the law and determine if you have a right to some portion of the records. NOW you can call the agency and ask how you go about getting the records – you should ask for them by name, and make sure you are talking to the right person.",
    "url": "http://localhost:4000/cronkite-docs/foia/public-records-outside-in.html",
    "relUrl": "/foia/public-records-outside-in.html"
  },
  "28": {
    "id": "28",
    "title": "R for journalism",
    "content": "R resources Here are several other sources for you to review if you want to try something different on your own: Matt Waite’s sports data analysis and visualization course from the Univ. of Nebraska. R for Data Science, an online textbook written by Hadley Wickham, inventor of the tidyverse. Intro to Data Science for the Social Sector, Jesse Lecy, from the ASU Program Evaluation and Data Analytics. This is much more a startup in R than a course in program evaluation. First 5 chapters of Sharon Machlis’ Practical R for Mass Communication and Journalism. The entire book should be available by early 2019.",
    "url": "http://localhost:4000/cronkite-docs/r-stats",
    "relUrl": "/r-stats"
  },
  "29": {
    "id": "29",
    "title": "Data sources and sourcing",
    "content": "Finding, acquiring and sourcing data An example: News21 “Hate in America” Data News 21 used Data the team didn’t use Finding data and documents Government agency sites News reports IRE.org tipsheets Academic articles Think tanks / interest groups Data collectors Vetting data provenance Data hunting exercise An example: News21 “Hate in America” In 2018, News 21 – the multi-university investigative reporting fellowship hosted by ASU’s Cronkite School of Journalism – chose “Hate in America” as its topic for the year. It was a risk because others had been reporting on the subject for more than a year, making it more difficult for News 21 to break new ground. It was also difficult because it became clear quite quickly that no one had documented every case of hate crimes or hate-driven incidents in the U.S. Data News 21 used That meant that the team had to find some creative way to quantify the problem. Some of the sources they used included: Raw data from the National Crime Victimization Survey, an annual survey of crime victims that asks whether hate was an element of the crime. Reporters Catherine Devine and Allie Bice could have used data from a report produced by the Justice Department, but instead analyzed the raw data in a new way to show that about twice as many incidents may have been motivated by hate than previously acknowledged. That analysis was thoroughly vetted by experts in the survey, in hate crimes, and in criminology. It also created a structure around the entire package and provided a newsy lead to the overview story A database created by a team of reporters who monitored two weeks’ of social media activity from users associated with white nationalists, new-Nazis and other far-right groups on sites including Twitter, Facebook, Gab and VK. It enabled Kia Gardener to write: News 21 monitored the daily social media activity of various far-right users, including white nationalists and neo-Nazis, from June 10 to June 24. Those tracked had more than 3 million followers combined. Reporters recorded and compiled more than 2,500 posts on popular platforms, such as Twitter and Facebook, and emerging social media platforms, including Gab and VK. About half the posts were directed at specific demographics or communities, from black Americans and Latinos to Jewish people and LGBTQ members.... Social Media: Where voices of hate find a place to preach, News 21, August 2018 Federal prosecutions of hate crimes under the various federal statutes. Reporter Lenny Martinez scraped all of the Justice Department’s hate crime-related press releases to find cases the government bragged about. Those cases were supplemented by a list of cases extracted from Westlaw federal case database. The team logged each case in a Google sheet to show what kinds of incidents were pursued by federal prosecutors, and where. ProPublica’s “Documenting Hate” project, which, with the Southern Poverty Law Center, tried to compile as many stories as they could about hate incidents. ProPublica’s database was a tip sheet, not a quantification. But it served one key goal of any data source: a source reporters could consult when seeking specific types of examples in specific locations. The FBI Uniform Crime Report’s Hate Crime series. They quickly learned that the data is seriously flawed because of non-response from local police departments and a squishy definition of what should be included. Another flaw was that others, including ProPublica, had thoroughly reported on those flaws and the trends in the data, meaning it failed the test of newsworthiness. Data the team didn’t use There were also sources that the team considered but didn’t pursue, sometimes because of the difficulty and sometimes because they were less useful to the project than expected: The Justice Department’s U.S. attorney case management system, which provided details on cases that the government chose not to pursue along with those they did. (A subsequent analysis showed that the vast majority of these cases were rejected by prosecutors, but vetting the analysis proved too difficult in the time available.) Databases of graffiti maintained by local police departments. This would have required public records requests to each department for records that usually aren’t clearly public. The team also contacted Google and other companies that publish street level images to see if it would be possible to isolate the hate symbols. Companies declined release images that their users had flagged as offensive. Historical questions from the Roper Center for Public Opinion Research and the General Social Survey that might have shed light on attitudes about race and religion over time. These proved to be difficult to match up over the years and didn’t really provide much insight. These are just some of the ways the News 21 team looked far and wide for any sources that could be methodically used to document their stories. As with any project of this type, the search often failed but along the way the whole team learned more and more about the topic and got to know experts in a way they wouldn’t have if they were just seeking quotes. Finding data and documents A big part of data reporting is finding, creating or acquiring records you can use electronically. Some sources of readily available data could include: Government agencies and open government sites Hobbyists and interest groups Data aggregators and data collectors Academic researchers who might share their data Microdata from surveys and some government programs, such as the Census, Medicare, the General Social Survey and several other standard sites. (This handout lists some of those standard sources.) Social data through API’s from Spotify, Twitter and other services. Details scraped from online data sources that aren’t available in bulk. There are also more difficult ways to find data: Public records requests Whistleblower leaks Home made databases created from documents, and free text or image document collections. Responses to a survey that you conduct yourself. Your own testing on issues such as water quality or soil contamination. For now, we’ll concentrate on how to find data that already exists in the public sphere and how to evaluate whether you can use it. Later on, we’ll work on how to identify data and documents you might want to acquire through public records requests. When you start on a project, you’ll usually rely on experts and advocates to lead you to a lot of the possible data sources. But you can also use these strategies to troll for interesting datasets that might make for good stories or practice. Listen to any caveats and warnings. You may decide that they’re not important, but you don’t want to be blindsided by them in the end. And be sure to ask what they would do if they were you – often, people who have expertise in data have story or project ideas that they can’t get funded or approved, and would be happy for someone else to do them. When you search using Google, try to use the advanced commands to more precisely hit your target. This tipsheet goes through all of the Google advanced search operators. The Verification Handbook for Investigative Reporting has a chapter from Henk Van Ess on advanced searching. Some of the sites he mentions no longer exist, but his strategy is sound. Government agency sites Try to guess what government agencies – state, local and federal – have an interest in your topic. Browse through their websites to find “Publications” or “Data and research”, or any searchable database. You’ll often find downloadable data there. Once you learn more, you can also evaluate how hard it will be to scrape the data you want. Don’t limit yourself to the jurisdications you care about. If one city or state has a good dataset, there is a strong chance that your local government will have the same thing. Look at federal agency sites to find a least common denominator database – they are usually compiled from more detailed state or local reports. Even if you can’t find the database, you might be able to find the name of a datset that is maintained internally in audits, footnotes of reports, or IT initiatives. Once you know a good agency to search, use advanced Google searches for filetype:csv or filetype:xlsx, and limit the site to an agency or city site to bring up datasets that they are letting users download. News reports One of the most useful sources to find the names of databases and their original sources is news reports that relied on the data, or refers to a data source quoted by experts. It doesn’t matter if you’re looking at your own area or others – most places have the same kinds of information collected and stories are similar across geographic areas. You should get good at using all of the resources as precisely as you can. That means getting very familiar with advanced searching in Google, and using LexisNexis and other news databases provided by the ASU library. These offer much more targeted searching than the usual Google search, and will result in much more on-point stories. When you find a good story, consider logging it in a spreadsheet or in doc, and identify: Who wrote it and when What government sources of data are explicitly mentioned. What analysis of that data was done by the news outlet, or what research it depended on. Any terms of art that seem to be used around your topic. For example, hate crimes are more frequently referred to as “bias” crimes in many articles – searching for “hate” might not surface them. IRE.org tipsheets Another source for information on news stories that used data reporting is IRE, which has two ways to search for more details: the ire.org tip sheets and story archive. Log into IRE.org and choose the tipsheets to look for guides from other reporters; choose the story database to look for stories on your general topic and then click into the form that the reporters filled out that go through their sources. You’ll often find a pair of them – a story, and a tip sheet – that were done by the same person the same year. (The database library is currently undergoing some review, so a lot of the data listed there could be out of date. But it might also point you to standard sources for data.) Academic articles Make sure to do a Google Scholar search for your topic. You will often find one or two researchers who have delved into your subject or a single source. This is often a great shortcut. For example, in the News 21 example, a search of hate crimes in Google Scholar identified an article called “Documenting Hate Crimes in the United States: Some consideration on data sources,” from APA PsycNet. Although this was specifically about sexual orientation and gender diversity, it cataloged the different ways that scholars try to document bias crimes. Once Devine settled on the crime victimization survey, another Google scholar search surfaced an expert on the survey who wrote about how it had changed over the years. He turned out to be the former chief of the Justice Department section that ran the survey, and was one of the project’s best sources. Another source led her to the book, “Statistics for Criminology and Criminal Justice.” One of the authors of that book also provided advice. Another value of this approach is that it will help you find the technical jargon for the topic you’re studying. It’s often very difficult to do literature searches without knowing that term. Think tanks / interest groups Try to find some interest groups that care a lot about your topic on all sides. They often have websites with recent research on your topic and might have experts you can consult. Take their advice cautiously because they often have a point to prove and are unabashed about twisting data to make their point. However, you can often use their raw data to draw your own conclusions. Some news organizations frown on this, so be sure to be transparent about who they are and what they’ve done. Another good way to use interest groups and think tanks is to get initial versions of public records from them while you wait for your own requests to be processed. At The Washington Post, we used an old version of a weapons trace database for a year while we fought the government for our own; we also used a copy of Agriculture subsidies acquired by the Environmental Working Group while we were waiting for our own public records requests to be completed. Sometimes, gathering the Tweets from advocates can provide a rich dataset, and it’s relatively easy to do. For example, I once used the Twitter posts from the Police Misconduct project out of the Cato Institute to get a list of all of the stories they’d compiled on the topic. Data collectors Several sites are trying to make businesses out of collected and maintaining databases. Others make available data that they have collected in the past. Be sure to look at the original source for any data you find there. You wouldn’t say a news article came from Google News or Lexis, and you wouldn’t say a dataset came from Google Data Search. If it’s not documented at all, you might have to contact the owner for more detail. Be careful of most of these. They’re often old, undocumented and poorly vetted. But they will give you a sense of what you might be able to get from a more reliable source, or give you ideas for your own data collection effort. enigma.com claims to have the “world’s broadest collection of public data” in a single place. For example, looking under United States -&gt; Arizona, you’ll see a database of Arizona liquor licenses, which was scraped from the state’s website two years ago. This is one of the rare curated sites. Enigma goes to great effort to document where and how they obtained their data, and to provide as much documentation as they can. They are also eager to work with journalists and will help however they can. data.world wants to be the Facebook or Instagram of data. It has both private and public accounts, and users upload data they want to share. This means it’s as varied as the people who are in it. If your newsroom is an AP member, you might have access to its data.world feed, which contains its curated and documented data that local newsrooms can use for their own stories. Some reporters also use data.world to store their public records. Some government agencies are posting their data directly to data.world. But in other cases, they’re undocumented hobbyists. Vet these the same way you would Google results. Journalists’ sites You can often find individual journalists or journalism organizations in various sharing sites, including Github (which doesn’t show up in default Google searches), data.world and other versioning. Look through their sites to see what they have collected – it’s there to share. Fivethirtyeight, ProPublica and the Los Angeles Times have particularly active data archives. Google data search is, well, the Google of data. In general, data search has limited sources and is more and more frequently logging data sets that are posted by state and local government sources. It makes no attempt to curate the search, though, so be cautious when you find something. One use for the dataset search is to see what other cities and counties have voluntarily released. When you see that, it often means your local or state government might have similar data you can request. For example, searching for police shootings brings up a dataset released by the Orlando Police Department, which contains far more detail than the same dataset released by Phoenix in 2018: Be sure to look for different terms meaning the same thing. For example, searching “use of force” brings you to completely different sets of data than “police shootings”. Vetting data provenance Before you even open a dataset, you should know how your dataset was collected, who it originally came from and how current it is. A future chapter will go through many of the ways reporters check data they’ve found for completeness, mistakes or other problems. At first blush, look for anything that precludes using the data because you can’t identify who is responsible for it or how it was collected. This is the same basic vetting you’d do on any source you hope to use. Look for: The original source. If you are getting it from a secondary source, look to see how hard it will be to get from original. If it’s from a secondary source, how reliable is it? Are you going to be comfortable crediting them for the data? If you can’t identify where or how the data was collected, you probably can’t use it. How others have used it and what criticisms were made of that use. The timeliness of the data. Anything more than two or three years old will be effectively useless for a news article. If it’s old, you should have a plan for how it will be updated. Data definitions, data dictionaries or record layouts. These are maps to the underlying data, and those definitions can prove difficult to understand. Data hunting exercise Try selecting a topic and log all of the data sources you can easily find on the subject. Be sure to log the link, any documentation you can find, and any concerns you have about the data. Finish with a proposal to use one of the sources you’ve found in a story.",
    "url": "http://localhost:4000/cronkite-docs/general/sourcing.html",
    "relUrl": "/general/sourcing.html"
  },
  "30": {
    "id": "30",
    "title": "Special",
    "content": "This is a set of specialized tools and tipsheets. They include things like scraping, mapping, visualization tools or data wrangling.",
    "url": "http://localhost:4000/cronkite-docs/special",
    "relUrl": "/special"
  },
  "31": {
    "id": "31",
    "title": "Story list",
    "content": "Readings Textbooks / handbooks About data reporting Newsroom numbers Visualation as a reporting tool Open source investigations Selected stories Scraping Backgrounding / public records Projects from me or my colleagues Textbooks / handbooks Computer-Assisted Reporting: A Practical Guide, by Brant Houston. This book is now in its 5th or 6th edition, even though it retains the quaint terminology of the 90s in the title. This version is from 2014. A newer one is on the way. “Data Literacy: A User’s Guide”, by David Herzog, 2015 “The data journalism handbook - an international guidebook to all forms of data journalism. “The Art of Access: Strategies for Acquiring Public Records”, by David Cullier and Charles N. Davis, 2010. A new edition will be published sometime in 2019, the author says. “Numbers in the Newsroom”, Sarah Cohen, 2nd edition. (But hey – arithmetic doesn’t change. It doesn’t matter which edition you get.) Samanth Sunne’s “Diving into Data Journalism” from the American Press Institute. About data reporting Mike Berens on the heart of data journalism, written when Bill Clinton was president. 60 data-driven ideas in 60 minutes, from Mary Jo Webster and Jodi Upton, NICAR 14. Serious Fun with Numbers, Dan Gilbert, Columbia Journalism Review. The story that won the Pulitzer is archived on the prize site: “The Money Prison”. “The Serial-Killer Detector”, The New Yorker, Nov. 27, 2017 about Tom Hargrove’s career as a data reporter and how it led to the Murder Accountability Project “A Data State of Mind”, Mary Jo Webster, Data-Driven Journalism, Sept. 2016 “Demystifying Data Journalism”, with Susan McGregor and Sarah Cohen at a Mashable conference. I think it’s 2012. Interview with data editor Janet Roberts on the Reuters approach to data reporting, 2018, from the Data Journalism Awards “How to Plan, Pitch and Do a Data-Driven Investigation”, Miguel Paz and Ryan McNeil, presentation from a CUNY course in 2016 (?). Many links are broken or private, but the ideas are still useful. Newsroom numbers Addressing Journalistic innumeracy, John Whibey, Journalist’s Resource website. Numbers in the newsroom: overcoming your fear of math. (Video of similar lecture) Visualation as a reporting tool Chapter from the original Data Journalism Handbook on finding insights through visualization, from Gregor Aisch, who is a longtime genius graphics reporter at the New York TImes. Financial Times’ “visual vocabulary”, as a pdf document or online at Github repo “Vizualization as a reporting tool”, from a conference at IRE (None of the links work on either the handout or the interview – don’t bother to try.) An interview with me from Poynter leading up to a conference The handout from the conference The slides that can provide the examples from the conference Slides from Peter Aldhous’s site on visualizing data for science investigations. Open source investigations “How to conduct an open-source investigation according to the founder of Bellingcat”, by Ned Bauman, New Yorker, August 30, 2018 Selected stories One way to keep up with stories is to subscribe to the Local Matters weekly newsletter, which curates stories done by more than 100 local newspapers, selected by reviewing their front pages on the Newseum site. Although it’s only newspapers, it gives you a good feel for the range of stories being done around the U.S. This is a fairly random set of stories that we often use in class to discuss how empirical journalism works. Sometimes we just read about a story – not the whole thing itself. There are probably thousands of stories that could be on this list but these are very well known or have some aspect that makes them good for class reading. The Bell, Calif., small-town corruption: “Is a City manager Worth $800,000?” and “Bell’s Money Flowed Uphill” “Cops among Florida’s worst Speeders”, by Sally Kestin and John Maines in the Ft. Lauderdale Sun-Sentinel, 2012. Dan Nguyen deconstructed the story for Stanford University and compiled a smallish dataset for practice. “Medicare Unmasked” from the Wall Street Journal (2014), in which reporters mined doctor billing records to find those who were fleecing the system. This link is to the Pulitzer Prize site because the Journal’s own site has kind of buried the big stories. Whether you use SQL or not, Dan Ngyuen’s tutorial using the data is a useful way to reconstruct a story like this. Elliot Jaspin’s story of finding racial cleansing in Census data. (very old!) Inside the Hidden World of Thefts, Joe Stephens and Mary Pat Flaherty, Washington Post, 2013. One of the first set of reporters to take advantage of new questions on the IRS 990 form available now in electronic form. Boston Globe 2018: “For some State Police, it’s a posting in Paradise,” by Kay Lazar and Todd Wallack. Be sure to read Todd’s tweetstorm on what it took to get this data, and how he got past bureaucratic and public records battles. Five-Thirty-Eight: Russian troll analysis on Twitter. Note the references to the original data, how it was collected, and the name of the project at Clemson – even if you don’t care about Russian trolls, it’s worth noting these kinds of resources for the future. ProPublica Illinois: “How Chicago Ticket Debt Sends Black Motorists Into Bankruptcy”, using several databases to show how Chicago’s aggressive ticketing is driving people into bankruptcy, by Melissa Sanchez and Sandhya Kambhampati, February 2018. The writing is a little rough, but the data work is solid and, most importantly, the stories were well reporting and identified. “L.A. is slammed with record costs for legal payouts”, Emily Alpert Reyes and Ben Welsh. Note the link at the bottom to github repo showing the data analysis. Scraping Todd Wallack from the Boston Globe on using a little programming to get to a story on liquor licensing. Be sure to read the story to understand how the data fits in with the story. (2018) Just wanted to say a few words about how data journalism can help reporters flesh out a story.&mdash; Todd Wallack (@TWallack) November 16, 2017 Backgrounding / public records Seattle Times’ award-winning coverage using backgrounding tools on deadline (2013?) WFAA’s coverage of a fertilizer plant explosion in 2013. You have to use your IRE login to get this, since WFAA doesn’t have archives. “I’ve sent out 1,018 Open Records Requests, and This is What I’ve Learned”, ProPublica Illinois, Sandhya Kambhampati, 2018 Projects from me or my colleagues I include projects that I worked on and that I oversaw as a data editor not because they’re so great, but because I understand the work that went into them. Sadly, The Washington Post’s site didn’t do a good job archiving some of them, so you have a pdf of the newspaper here. A Death in St. Augustine from Frontline / NYT as an example of weaving data into a narrative. On the Times’ site, it was published as Two Gunshots on a Summer Night, with a secondary story that is a little more data-wonky. Note the ending comparison – those came from our various datasets. (2013) “Police Chiefs, Looking to Diversify Forces, Face Structural Hurdles”, Matt Apuzzo and Sarah Cohen, The New York Times, Nov. 7, 2015. This is a routine story, not a project, which makes it a little easier to understand. The cleaned up data for this project is a good practice dataset, and you can see how I found Chief Riley. (2015)",
    "url": "http://localhost:4000/cronkite-docs/general/story-list.html",
    "relUrl": "/general/story-list.html"
  },
  "32": {
    "id": "32",
    "title": "Visualization as a reporting tool",
    "content": "Visualization as a reporting tool The slides and a tip sheet are included in this folder. Tools for data visualization### The R programming language’s ggplot2 is probably used most frequently by reporters to get quick visualizations that won’t be published. It’s each to make some of the most common charts, graphs and maps with default options that take advantage of a century of research into visual perception. It has a recent add-in called Shiny to make interactive visualizations, something it was missing in the past. D3 is used by reporters who already know Javascript (not many). It’s the language that has pretty much replaced Flash and Activescript. Most reporters, as opposed to those who produce visualizations for publication, use simpler tools. Keep in mind that the free, online versions of some of these tools will make your data public by default. Excel’s implementation of sparklines. They are an amazingly simple and effective way to see trends quickly. Tableau Public is a small-scale version of the expensive Tableau software used in business and industry. The more powerful desktop version is free for IRE members, but about $2,000 a seat if you pay. ManyEyes has a similar public approach, but is a little easier to use for very small scale visualizations. Google Fusion Tables have the advantage of geocoding (putting addresses on a map when you don’t know the coordinates.) It’s a little confusing, but there is a big user base and a lot of help. Timeflow, an alpha level product created at Duke University for exploratory investigative timelines and calendars. It will run off of a hard drive, but your security settings may still block it. Most other timeline programs are for publication rather than exploration (at least the ones we can afford) Simile Exhibit, a project out of MIT that attempts to make it easy to make timelines, Google maps and small searchable datasets in one place. It’s good for data that won’t change a lot, since you have to have a specific data format for it. It’s not very actively supported and still feels like an academic project, but it generally works. Visualizing text Voyant Tools lets you upload text or pdfs and get a high-level view of the contents. Here’s an example using Martin Luther King’s “I have a Dream” speech, or [this one] (http://voyeurtools.org/?corpus=1411347124307.8286&amp;stopList=stop.en.taporware.txt) from three Chicago Police Board decisions. ManyEyes has several visualizations that are just for text. The same example Wordseer. I haven’t tried this yet – it’s just recently available to the general public. But I’ve seen demos and it looks interesting. Although it’s not really visualization, ClearForest Gnosis, a plug-in for Mozilla, is helpful for color-coding the entities on your documents. There are a lot of other text visualization experiments in the digital humanities, especially history, linguistics and literature graduate programs. Keep an eye on the projects at the Digital Research Tools site, which is constantly updated.",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/viz/viz-reporting.html",
    "relUrl": "/assets/docs/viz/viz-reporting.html"
  },
  "33": {
    "id": "33",
    "title": "Workshops",
    "content": "Holding page for index of workshops.",
    "url": "http://localhost:4000/cronkite-docs/workshops",
    "relUrl": "/workshops"
  },
  "34": {
    "id": "34",
    "title": "Filter and sort",
    "content": "Sorting and filtering to find stories A sorting miracle Sorting and filtering as a reporting tool Create a data table from your spreadsheet Sorting Adding fields to the sort Filtering Free text and date filters Creating your own flags with filters FAQ How do I turn off all of my sort and filters Where is the button to filter columns? I’m getting weird questions and alerts about sorting I want to get rid of my data table Show me a video Data files used in this tutorial: Salaries for city workers in Phoenix (source: Phoenix Open Data portal) Opioid-related EMS calls in Tempe, Az. (source and documentation: Tempe Open Data Portal) A sorting miracle After Ferguson, Mo., police killed Michael Brown in 2014, advocates and journalists began examining the racial and ethnic gap between police departments and the communities they served. The New York Times found a 7-year-old survey conducted by the Justice Department that allowed it to compare the data for major cities in a standalone graphic that it published later that year. When newer data reflecting departments’ makeup in 2012 was released a year later, Matt Apuzzo and Sarah Cohen hoped it would show some differences. It didn’t. So they were left trying to find news in the data that was clearly of public interest. After matching up the demographics of police departments with their cities, Cohen started sorting, filtering and Googling. Could there be news in the outliers on the list? Which departments most closely represented their communities? Which ones had unusually large gaps? She quickly stumbled on telling anecdote to frame the story: Inkster, Mich. had one of the least representative departments in the country, and had recently hired a new police chief to help mend the department’s fraught relationship with its largely African-American community. Where had he come from? Selma, Ala., one of the most representative police departments in the nation. Interviews with the chief, William T. Riley III, suggested one reason for some cities’ disparities: there was no state or federal money to pay for training new police officers. “There’s no doubt in my mind we have good police candidates in this city. No doubt,” said Chief Riley, who is African-American. “How hard can it be? You’re telling me everybody here can’t be a police officer?” The story, “Police Chiefs, Looking to Diversity Forces, Face Structural Hurdles” helped explain the persistent gap between the makeup of police in some areas and the communities they served. Sorting and filtering as a reporting tool Reporters frequently sort and filter for both stories and for data integrity and cleanup. One of the most common uses of sorting and filtering comes with spreadsheets you build yourself. If you keep your data tidy and make sure that each column has the same type of information – particularly dates – it does’t matter if you’ve had to collect it piecemeal and out of order. Sorting can magically turn it into a chronology, useful for spotting holes in sources’ stories and putting background information in context. If you build fact-checking and sourcing into your spreadsheet, filtering can hide from you any material that isn’t publishable while you’re writing, making it less likely to let poorly sourced or secondary source information into your story. Sorting and filtering can also: Show you rows containing the highest and lowest values of any column. That can be news or it can be errors or other problems with the data. Let you answer quick “how many?” questions, with a count of the rows that match your criteria. Together, sorting and filtering can narrow your dataset to just the items you want to examine more closely in your reporting. Create a data table from your spreadsheet In Excel, you may want to create a “data table” from your spreadsheet to help make sure you’re always sorting and filtering the correct rows. This is a way of enforcing just a little structure and rigor onto a spreadsheet, which usually lets you do whatever you want. There are several benefits to data tables: Formulas automatically copy throughout your data, whether or not there are empty values next to them. Sorting and filtering are always done on the proper list of rows, once you’re made sure to define the table correctly. You can use column names rather than addresses to reference a field. The column names always stay at the top of your screen, making it unnecessary to freeze panes (which can make navigation more difficult) It also looks better. Select your entire table, using Shift-CTL-* or by selecting the corners, then choose your table style: Sorting Sorting means rearranging the rows of a data table into a different order. Some reporters take a conceptual shortcut and call this “sorting columns”. That thinking will only get you into trouble – it lets you forget that you want to keep all of the rows in tact while changing the order in which you see them. In fact, in other languages it’s called “order by” or “arrange” by one or more columns – a much clearer way to think of it. In Excel, look for the sort options under the Data tab at the top of your screen. In this case, sorting from Largest to Smallest gives you the highest-paid employees in the Phoenix city government. Gives you this: Adding fields to the sort You can sort by more than one column to see just the information you want at the top. When you add conditions, Excel sorts by the first column, putting the rows with lowest (or highest) values at the top. When those are the same, it will sort within that group by the second item you mention. And so on. So let’s say I want to see all of the “probable” opioid-related emergency calls in Tempe that were treated with Narcan, ordered chronologically. We’re in luck, because “Yes” comes at the end of the alphabet, letting us sort “From Z to A” to get them on top. Add more levels using the + sign in the dialog box: Notice that Excel understood that the first two fields were character fields, so it asks you if you want A-&gt;Z or Z-&gt;A (ascending or descending). But when it came to the incident date and time, it understood these were dates, so it asks if you want “Oldest to newest” instead. (The little arrows on the headers showing you they are sorted only happen when you have your data formatted as a table.) Filtering Filtering means picking out only some of the rows you want to see based on a criteria you select in a column. Think of it as casting a fishing net – the more filters you add, the fewer fish will be caught. The filtering buttons are shown when you create a data table. If they’re not, look in your Data tab at top press the button that looks like a funnel: Looking through your filters is an easy way to see the range of information you have in your dataset. For example, pulling down the button for filtering on the Narcan flag, you see only the options for “Yes” and “No”. But if you click on the dropdown box for the incident date and time, you’ll first see just the years, which you can expand to months, days, and even hours. Check and un-check any of the boxes that you want. Here’s a filter that picks out just the Narcan cases of probable overdoses, sorted chronologically: Excel tells you how many of the rows match your filter – 202 of 650 in this case, shown at the very bottom of the screen. (That count goes away sometimes after a few minutes. Just turn the filter on and off again to see it.) It also gives you several clues that you’re looking at filtered data: The row numbers have turned blue, and and there are dark lines between them when rows are skipped; the drop-down arrows now show a little funnel, indicating that you are using that column for a filter. Free text and date filters When there are long phrases in a field, you can just type a word you’re looking for into the box: And when you have numbers or dates, you’ll get different choices. We don’t really have any numbers on this dataset to work with, but here are some choices for dates: Creating your own flags with filters Just from this example, you see why analysts often create some seemingly repetitive information. In this table, there is a list of medications given by the EMS crew, and Narcan might be one of them. To make it easier to analyze, they have also added a “Yes/No” flag just for Narcan, which we used for sorting and filtering. You can create your own flags using filters. (There are other ways to accomplish the same thing using formulas.) I wanted a simple “Yes” / “No” field for oxycodone. Going back to the text filter, we can see that there were 52 rows that contained a mention of Oxycodone in the “current medications” field. When I add a new header to the end, Excel automatically appends it to my data table. I can enter “Yes” into the first row, and copy down, and it will only fill in the visible cells. Remove your filter from the medications list, and it looks like this: Now I can filter for all of the blanks in that field, and fill them in with “No” to create the rest of the flag: Just be sure you document HOW you made your filter. What did you do with “(Blanks)” or anything that didn’t make sense? You should be able to recreate your flags exactly whenever you make them, and it’s easy to forget exactly what you did. FAQ How do I turn off all of my sort and filters In the data tab, chose “Clear” (the funnel with the red “X”) to remove all of the filters and sorts on your table. Where is the button to filter columns? Sometimes you don’t want to see all of your columns – there are too many and they’re getting confusing. There is no column filter in Excel. (You’ll see how to filter, or “Select”, columns from a dataset in standard programming languages later.) Instead, you can hide the columns you don’t want to see. When columns and rows are hidden, they generally won’t copy to a new sheet. I’m getting weird questions and alerts about sorting Slow down and read the alert. There are two common types of alerts in sorting, since it has the potential to wreck your spreadsheet. The first comes if you selected an entire column, and then just hit the button that says “A-Z” with the arrow. Excel won’t let you do that if it’s formatted as a table, but it will if it’s just a normal spreadsheet. This alert asks you if you REALLY want to sort only the column you’ve selected, separating its meaning from the rest of the rows. The answer is NO. Always. Expand the selection as Excel wants you do to by default. The other comes when you have numbers that are treated as text. This is a tricky question, and a properly tidied spreadsheet should avoid it most of the time. If you have the same type of data in each column, the answer to this question shouldn’t matter. If not, neither one will give you what you want. I want to get rid of my data table You can revert to the a plain old spreadsheet by selecting any cell within your table, then looking for the “Table” tab at the top of your screen. Choose the option that says “Convert to Range”. Show me a video This video goes through many of the details of sorting and filtering:",
    "url": "http://localhost:4000/cronkite-docs/excel/xlguides/xl-filtersort.html",
    "relUrl": "/excel/xlguides/xl-filtersort.html"
  },
  "35": {
    "id": "35",
    "title": "Excel formulas",
    "content": "Formulas in Excel There are lots of good tip sheets and reminders of how to use formulas in Excel. This page will walk you through an example, but consult these excellent resources if you get stuck: If you’re brand new to spreadsheets, or feel uncomfortable, start with Mary Jo Webster’s “Beginner Excel” tutorial. Jaimi Dowdell’s quick list of Excel formulas. It’s written for Windows, but nothing is different on a Mac in this list. The math review is going to use the City of Phoenix’s budgeted spending for the 2018 fiscal year, compared with previous years. (Source: https://www.phoenix.gov/budget/annualbudget) Files for this tutorial Get into good habits Common spreadsheet arithmetic Check the government’s math with SUM Change in spending Percent change Parts of a whole: percent of total While we’re at it: two kinds of averages The final spreadsheet FAQs Excel won’t let me copy my formula Should I use average or median? My percents are small numbers with decimal points Mac Users: There’s a little image of a cell covering up my spreadsheet Files for this tutorial A data diary for the processing that went into this spreadsheet for this tutorial. The Excel spreadsheet for this tutorial. Get into good habits Right-click on this link to the Phoenix budget summary that I prepared for you, and choose “Save As” to keep a copy on your computer rather than opening it directly into Excel. Once you’ve done that, right-click on the Download area of your browser and look for the file on your computer. Don’t double-click – we’re going to ask you to get out of that practice as much as possible, understanding that it’s a natural instinct. Start your documentation worksheet or document and describe, in plain language, every question you’ve asked, every answer and its source, and every step you’ve taken. Save a working copy with some sort of sequential name in a folder you can find again. Is there documentation that provides the source of the data, and the meaning of each column? Can you get it from the original document or dataset? Check your corners - how far to the right and how far down does the data go? Is it a contiguous square? Is it “tidy”? Does every column refer to one thing, and every row one instance of that thing? Does it have exactly 32,767 or 65,536 filled in rows; does it have exactly 256 filled in columns? Does it have unique identifiers for each row? If not, make them in a way that will guarantee you can re-sort back into the original order. Are totals or notes mixed into the same contiguous area as detail? If so, separate them from the data. Common spreadsheet arithmetic The budget document shows three years’ of data: The actual spending in the fiscal year that ended in 2016; the spending that was estimated for the end of fiscal year 2017; and the proposed spending for fiscal year 2018. The first page of the document shows these amounts for broad spending categories. You may want to widen the columns and format the numbers before you start: Check the government’s math with SUM Our first job is to make sure the government has provided us data that adds up. To do that, we’ll SUM all of the departments’ spending. To add up the numbers from 2016, enter the following formula in cell C11, just below the number provided by the government: =SUM(C2:C8) and hit the enter key Copy that formula to the right. Notice how the formula changes the addresses that it is using as you move to the right – it’s adjusted them to refer to the current column. What’s wrong? The numbers for the budget 2018 don’t add up. (Hint: look at the page called “notes” for an explanation.) Change in spending The increase or decrease in projected spending from 2017 to 2018 is just the difference between the two values, beginning in cell F3 new-old, or =E2-D2 When you copy it down, note how the references to each row also adjusted. In line 3, it’s E3-D3, and so on. Excel and other spreadsheets assume that, most of the time, you want these kinds of adjustments to be made. Percent change We can’t tell the rate of growth for each department until we calculate the percent change from one year to another. Now that we already have the change, the percent change is easy. The formula is: ( new - old ) / old We’ve already calculated the new-old part, so now all that’s required is to divide by the old value. In grade school, you also had to move the decimal place over two spots, since the concept of percent change is “out of 100”. Excel formats will do that for you. Remember, it’s always (new-old)/old , NOT the big one minus the little one. Doing it correctly, the answer could be negative, meaning the value fell. When you’re done, you can format the answer as a percentage to get it into whole numbers. Until you get used to it, there’s no harm in doing these calculations step by step. Excel won’t complain if you have extra columns. You can always hide them. It’s also worth comparing the picture you get by looking at raw numbers vs. percentages. In our case, the budget for public safety is expected to rise by a whopping $102 million, but it’s a smaller percentage increase than other, smaller departments. Parts of a whole: percent of total We’d also like to know what portion of the total spending is eaten up by each department. To do that, we need the percent of total. In our case, let’s use the total that the government gave us. In practice, you’d have to decide what to do if your figures didn’t match those provided by officials. You can’t assume that the total is wrong – you could be missing a category, or there could be a mistake in one of the line items. The formula for percent of total is: category / total Again, Excel will multiply by 100, or move the decimal place over for you once you format. But you have a problem: You either have to type in each row, or you get something like this if you try to copy: Excel has done its magic, adjusting the location of both the numerator and the denominator when you copied. You don’t have to type in each formula one by one, though. Instead, you’ll use anchors, known in spreadsheets as “absolute references”. Think of a dollar sign as an anchor or stickpin, holding down the location of part of your formula. If you put the stickpin before the letter in the formula, it holds the column in place. If you put it before the number, it holds the row in place. If you put it in both places, it holds the cell in place. So our new formula for the percent of total is: While we’re at it: two kinds of averages Although it doesn’t make a lot of sense in this context, we’ll go ahead and calculate the average or mean size of each department, and then calculate the median size. Simple average, or mean A simple average, also known as the mean, is skewed toward very high or very low values. Its formula is sum of pieces / # of pieces that were summed But in Excel, all we need is the word AVERAGE: =AVERAGE(C2:C9) Median There’s not really a good formula for the median. It’s the middle value of a list, once they’ve been put in sorted order. (If there is an even number of values, it’s the average of the two middle values.) This just treats a very high or very low value as just another number, and it doesn’t affect the summary very much. For example, if we have five people with the following incomes: $8,000 $10,000 $12,000 $15,000 $500,000 The average, $109,000, will not be a good summary of the list. In fact, no one on the list makes anything like that. But the median, $12,000, reflects the middle of the pack. With America’s income inequality, this is common in anything measured in dollars like home values or incomes. In Excel, you can get the median of a list of numbers by just using the formula, MEDIAN() = MEDIAN(C2:C9) The final spreadsheet At this point, write out a few questions you might want to ask an official if you only have a few minutes. Now that you have some data that might point to news, you can use it to ask the official to confirm your analysis and explain the underlying reasons. FAQs Excel won’t let me copy my formula Make sure your formula is locked in by either hitting “Enter” or “Escape”. This is a common problem if you’re in the habit of double-clicking instead of selecting a cell. There are a lot things you can’t do while Excel thinks you’re still entering information. Should I use average or median? It depends. Averages are easier to explain but can be misleading. Usually, if they’re very different, median will be a better representation of the typical person, city or department. Averages in these cases are more like totals. My percents are small numbers with decimal points Use the format as a % button to move the decimal point over two places and insert the percentage symbol. Mac Users: There’s a little image of a cell covering up my spreadsheet I have no idea what this is, but it happens. Save your spreadsheet, close it and then re-open. It usually goes away.",
    "url": "http://localhost:4000/cronkite-docs/excel/xlguides/xl-formulas.html",
    "relUrl": "/excel/xlguides/xl-formulas.html"
  },
  "36": {
    "id": "36",
    "title": "K-pop",
    "content": "How much fun is k-pop? In 2018, Lisa Travis, a student at the Walter Cronkite School, wanted to know more about her musical passion: k-pop. We worked to get the 11 characteristics of top k-pop stars’ biggest hits into a dataset using the Spotify API. Now we can use her data to think about how numbers are useful in operationalizing concepts like “boring”, “fast”, or “sad”. Anyone with a Spotify account can get access to its API, or Application Programming Interface. We used several R programs to put the data together – one pass got the most popular artists in Spotify by genre; the next found their top tracks; and the last rounded up the details about those tracks. Most of the metrics run from zero to 1. For example, a valence of 1 means the song is really, really happy. A valence of 0 means it’s really, really sad. Read this analysis of one person’s playlist to learn more about what the different measures are, and how he decided to check if his music was boring. We’re going to go through some of the same measures and statistics to see how k-pop does in a similar analysis. (We’ll skip the machine learning prediction of whether a song is his or hers.) Data and documentation Import and interview Turn off “named” formulas Calculate some summary statistics Popularity Key Operationalize some concepts by creating new fields On your own Footnotes Data and documentation kpop_spotify_tracks.tsv is a tab-separated text file exported from R containing the k-pop tracks. (There were some duplicates in the original dataset – I’m not sure how they got there, but they’ve been eliminated.) Most elements are defined on the Spotify API track details documentation. The tracks were selected from those that can be played by subscribers in the U.S. Additional fields came from other parts of the Spotify API. They are: field description artist_id Spotify’s artist identifier, used to look up tracks all_genres Every genre that is associated with the artist, in a semicolon-delimited list popularity A value between 0 and 100, assigned by Spotify, with 100 being the most popular follower_count The number of Spotify followers for the artist track_id Spotify’s track id. The original scrape of the data resulted in some duplicates, often because they were available in several different versions. The most popular version was kept. track_name   track_popularity A value between 0 and 100. When a track was listed more than once, the most popular version was kept. Spotify says it’s based on an algorithm that takes into account the number of plays and how recent those were. These values are aggregated to come up with artist popularity. track_length in milliseconds Import and interview By using a .tsv file instead of a .csv file, you have more options when you open the dataset. Don’t double-click on it. Instead, start up Excel and use File / Open to open it up. That will bring you a diaolog box that asks how you want to deal with it. One of the options is to tell Excel what character set to use. Choose Korean (Mac). If you leave it at the default, all of the Korean characters will turn into mathematical symbols! Take a few minutes to interview your data. Some possible first steps: Convert the data to a table, and format some of the numbers to make them easier to read. (You’ll notice some of the numers are showing up in scientific notation, like -5.35E-06. Scientific notation is used for very, very large and very, very small numbers. This means that the number is 5.35*10-6, or 0.00000535.1 Insert a column and add an ID so you can get back to the original order. Play with your filters while studying the documentation of what each column means. What data type is each column? For example, “key” is actually something we would normally treat as words, or categories. “Mode” is a yes-or-no field. Many of the measures run from 0 to 1, with 1 being 100% of the characteristic, and 0 being none of it. Get a sense of how much each value varies – are they all around the same level, or is there some range to the values? If a measure doesn’t vary enough, it won’t be interesting – there’s no news. Try sorting by various fields. Are the most danceable songs also the happiest with high valence? Turn off “named” formulas Excel usually uses the column names in tables when you create formulas, which causes problems when you want to copy left-to-right. Turn off that option under Excel Preferences: Calculate some summary statistics It’s useful to know how much fields vary within themselves. You can calculate various statistics to get a good sense of this: Measure Excel formula Meaning Mean =AVERAGE(range) the simple average of a column of values Mode =MODE(range) the most common value Standard Deviation =STDEV.P(range) A standard measure of variance. Rule of thumb: If it’s larger than the mean, the numbers are vey spread out. If it’s very small, there is almost no spread. This is a useful shorthand to see how well an average summarizes a group of numbers. Maximum =MAX(range) the highest value 75th percentile =PERCENTILE.INC(range, .75) three quarters of the values are at or below this level Median =MEDIAN(range) the middle value, where half are higher and half are lower, or the 50th percentile 25th percentile =PERCENTILE.INC(range, .25) one quarter of the values are at or below this level Minimum =MIN(range) the lowest value These are typical “descriptive statistics” that help you quickly sum up a set of numbers. Let’s take the track popularity field. The mean popularity score is 57, which is more that the average across all songs in Spotify. That makes sense because we chose the most popular kpop artists. The standard deviation is 9.6 (rounded to 10), meaning that two-thirds of the songs are between about 47 and 67 on the popularity scale. That’s not very much variation, but it’s enough to get something interesting out of it. The most popular track scored a 91, and the least popular scored 22. Here are two histograms, which show the number of songs that fall between the values shown on the bottom. The first one, of popularity, range from 0 to 100, and our songs are on the high end but have a fairly typical distribution. But the key, measuring pitch, has no pattern – there are just about an equal number of songs in each key. This tells you that there is no typical answer to key – an average doesn’t mean anything. Popularity Key Operationalize some concepts by creating new fields One research concept that you can use in journalism is the idea of operationalization. It’s a way of measuring something that seems immeasurable. In our example blog, the author operationalized the concept of “boring” by combining several variables: energy, danceability, tempo and loudness. However, these measures are on a different scale, so he had to bring them back to the same level by converting a few of them. The lower the measure, the more boring the song is. To make that easier to remember, let’s call it “fun-ness” fun-ness = (loudness + 60) + tempo + (energy * 100) + (danceability * 100) (Spotify has changed how it measures loudness since this blog was written. It now ranges from -60 to 0, so I added 60 to the measure so it would go from 0 to 60) Doing this, our most “fun” song is HANGSANG, from j-hope. Our most “boring” song is TAEYON’s “All with you” (I think the boring measure worked pretty well. I’m not so sure about the fun one!) On your own Think of a feeling you’d like to measure listening to the songs – then try to think of what variables, put together, might help you operationalize it. Footnotes Scientific notation can actually cause you problems in computers. That’s because it’s an efficient way to hold numbers, but it also loses precision. In fact, in some languages, 0 doesn’t equal 0 because of it! It’s just something to be aware of when you get odd results.",
    "url": "http://localhost:4000/cronkite-docs/excel/practice/xl-kpop.html",
    "relUrl": "/excel/practice/xl-kpop.html"
  },
  "37": {
    "id": "37",
    "title": "Simple data diary for Excel",
    "content": "A simple data diary for Excel A simple data diary for the beginning of the Excel math review module might look something like this: Data source Original file downloaded as a xx-page PDF from the Phoenix city budget site at https://www.phoenix.gov/budgetsite . The link was https://www.phoenix.gov/budgetsite/Budget%20Books/SummaryBudget2017-18.pdf. It’s undated as far as I can tell. The key tables were on pages 182-183 – that is listed as the “Expenditures” by program. Using Acrobat, extracted out those pages into a new file, ~/Documents/examples/ phx_budget_2017_table.pdf I tried to extract the tables using different programs: Able2Extract, Adobe Acrobat, Tabula. Then I realized that it is an image, not a text, pdf. Went back and OCR’d it using Adobe Acrobat, but when I sent it to a csv, it was a mess. I decided to type in the numbers myself rather than try to clean it up. The original spreadsheet Created a blank spreadsheet called ~/Documents/examples/phx_budget_summary.xlsx . Added a notes page to document where I got it from. categories sheet: Typed in the numbers from the 2016 actual, 2017 estimated and 2018 budget category totals. Selected them and checked against the totals shown at the bottom of the page. They were right, except for 2018. Then I realized I hadn’t typed in the “contingency” line. I decided not to type it in at all. The reason is that, the way I understand it for now, this is money they hope not to spend. It’s a reserve fund for something that’s unexpected that comes up. There’s no category for it in the past because if it was spent, it went into the proper category. Add to the TO DO list to find out if that’s the right way to treat it. Did the same thing for the individual programs on another sheet, but entered the categories into their own column, in a tidy fashion. Checked against the originals. They matched. Made sure to un-format any numbers so that if I want to export to another program, they’ll come in correctly. This spreadsheet won’t be changed unless the data is updated. Cleanup Formatted the numbers as dollars. Widened the columns Added a unique identifier to the detail rows in categories in case I need to sort. Created a row that checks the sums of the detail against the sum provided by the city. Saved the spreadsheet as phx_budget_summary_sc01.xlsx Questions What is the money for contingency? How has it been used in the past? What does “estimated” mean for 2017 – don’t they know how much was actually spent? When was this created? Is there any reason that comparing the estimated 2017 to budgeted 2018 would be misleading? Keep going You can finish the rest yourself, but the key is that everything you’ve done is logged, and anyone reading this can follow what you did. Including yourself.",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/xl-mathreview-datadiary.html",
    "relUrl": "/assets/docs/xl-mathreview-datadiary.html"
  },
  "38": {
    "id": "38",
    "title": "Grouping with pivot tables",
    "content": "Grouping with pivot tables “Group By” is a universal concept in data – it means counting, summing or otherwise summarizing raw records into by group. If you have a list of people and the high school they attended, a group by operation will give you the number of people by high school. In Excel, grouping is done with pivot tables. Here are two excellent tutorials on pivot tables. There’s no need to re-create them here. The Data Journalism Textbook project’s Group By tutorial for Excel by Matt Waite. To get the data, go to the Data page in that project and right-click on the mountainlions.csv file. Then choose “Save As” to prevent it from opening in Github. Mary Jo Webster’s tutorial is pretty comprehensive with lots of screen shots. For a detailed walkthrough on a season of baseball scores, see the “Arizona Diamondback scores” tutorial under “Excel practice” Finding good practice datasets can take some practice. Here are a few that offer good potential. If you want to find your own, try to find data sets that contain individual records, not statistics. Baseball salaries Traffic stops obtained from St. Anthony, Minn., after a police officer fatally shot a black motorist named Philandro Castile.",
    "url": "http://localhost:4000/cronkite-docs/excel/xlguides/xl-pivot.html",
    "relUrl": "/excel/xlguides/xl-pivot.html"
  },
  "39": {
    "id": "39",
    "title": "Excel refresher",
    "content": "Spreadsheets and reporting Re-learning Excel from the ground up The spreadsheet grid Mouse shapes Selecting cells and areas Entering data Locking in headings Formatting tricks Getting started with a dataset First steps Interview your data Headings Unit of analysis Row numbers Example Keyboard shortcuts Spreadsheets and reporting You may have learned how to use Excel or Google Sheets in grade school or high school to make charts or present tables, but reporters use spreadsheets in different ways. The key reasons for reporters to use a spreadsheet are: To create our own databases that so that we can sort, filter and count events. Examples include a long-running court case; the details of each opioid death in a city; a list of police shootings and their documents; or even a list of your own public records requests or contact log. To use data created by others for fast, simple analysis and data cleanup. Many government agencies provide their information in spreadsheet form, so you’ll need to get used to using it. To perform simple, straightforward analysis on data and share with team members. This is becoming less common as more reporters learn programming languages, but it’s still common in newsrooms to share data, especially through Google Sheets. We’re going to work with Excel because it is still far more robust than Google sheets for much of our work. Google sheets are difficult to navigate, have limited options for filtering and have other limitations. However, the basic concepts still apply and Google sheets are better for some tasks, including interacting with Web pages and working collaboratively with team members. Simple lists are much easier to share than various Excel attachments or Dropbox files. Some reporters flinch at typing in 30 or 100 entries into a spreadsheet. You shouldn’t. If you learn to take notes in a structured way, you’ll always be able to find and verify your work. If you try to calculate a sum of 30 numbers on a calculator, you’ll have to type them all in at least twice anyway. And getting used to these easy tasks on a spreadsheet keeps you fluent for when you need to do more. Re-learning Excel from the ground up The spreadsheet grid When you start up a spreadsheet, you’ll see letters across the top and numbers down the side. If you ever played Battleship, you’ll recognize the idea – every little square, or cell, is referenced by the intersection of its column letter and row number: B2 is the cell that is currently active. You can tell because it’s outlined in the sheet and it’s shown on the upper left corner. Mouse shapes BFWPS: The Big Fat White Plus Sign. This is the default shape, and you can never get into trouble when you see it. The Copy Tool, or the thin black cross. When you see this, you’ll copy anything that’s selected. This can be good or bad. The Evil Hand. (In Windows, this is the Evil Arrow). If you use this symbol, you will MOVE the selection to a new location. This is very rarely a good idea or something you intend. Selecting cells and areas Spreadsheets act only on the cells or regions you have selected. If you begin typing, you’ll start entering information into the currently selected cell. To select: Hold the BFWPS over the cell and clice ONCE – not twice. Check the formula bar to make sure you’ve selected what you think you’ve got. You can also look at the bottom right of your spreadsheet for more information. To select a group of cells and act on them all at once: Hover the BFWPS over one corner, click ONCE and drag to the diagonal corner. Make sure the Evil Hand is nowhere to be seen. The entire area will be shaded in except for the currently selected cell. Look at the upper right corner to see how many rows and columns you selected. To select a column or row : Hover the BFWPS over the letter at the top of the column. For a row, hover it over the row number in the margin Entering data Select the cell and start typing. The information you type won’t be locked into the cell until you hit the Return / Enter key, or move your selection to another cell. Hit “Escape” to cancel the entry. You can’t do a lot of things while you’re editing, so if you have a lot of greyed out menu items, look at your formula bar to see if you are still editing a cell. If you’re having trouble getting to a menu item or seeing the result of your work, try hitting “Escape” and try again. You may not have actually entered the information into the sheet. Locking in headings As your spreadsheet grows vertically with more rows, you’ll want to be able to see the title all the time. When it grows vertically with more columns, you’ll probably want to see what row you’re looking at by name whenever you can. This is called “Freezing Panes” – you freeze part of the page so it stays in place when you move around. Select the corner that you want frozen. For example, if you want the first three columns frozen (A:C) and the first row frozen (1), then select the cell in D2. This is the first cell that will move, and everything to the left of it and above it will stay on the screen. Formatting tricks Use the buttons or the format dialog box to make numbers easier to read. If a column is filled with a lot of text, select the column and look on the Home ribbon next to the formatting area for “Wrap Text”. This means that when you double-click to widen a column, it will get taller, not wider. This is good when you need to save valuable real estate on the screen. Getting started with a dataset SLOW DOWN! Don’t do anything until you understand what you have in front of you and can predict what your next mouse click will do to it. Most data we encounter was created by someone else for some purpose other than ours. This means that you can’t assume anything. It may not be complete. It may be inaccurate. It may mean something completely different than it appears at first blush. First steps Document where you got the spreadsheet and how you can get back to the original. Read anything you can about what it contains. Look for documentation that comes with the data. Save the original into a safe place with its original name and metadata. Work on a copy. If the spreadsheet shows #### instead of words or numbers, widen your columns. If it shows 7E-14 or something like that, format them as numbers, not “General”. Check your corners – look at the top left and bottom right. Is the data all in one area? Are there footnotes or other non-data sections mixed in? We’re going to want to fix that later. Interview your data Headings The most fraught part of data reporting is understanding what each column actually means. These often have cryptic, bureaucratic names. You may need to go back to the source of the data to be sure you actually understand them. If your data doesn’t have any headings, that’s going to be your first priority. In effect, you’ll need to build what we call a data dictionary or record layout if one hasn’t been provided. Unit of analysis A unit of analysis refers to the items that are listed in the rows of your dataset. Ideally, every row should be at the same unit of analysis – a person, an inspection, or a city, for example. Summaries should be separated by a blank row, or moved to a different sheet. Row numbers The data was probably given to you in some sort of natural sort order. Different computer systems sort differently – some are case-sensitive, others are not. Some put numbers before letters, others don’t. The order of the data may depend on a column you don’t have. For example, say an agency gave you the data in the order it came into the system, but didn’t give you that ID number. If you don’t do something now, you’ll never be able to get back to the original order, which could have meaning for both the agency and for fact-checking. Example These first steps, along with adding an ID row, are shown here. You can follow along with the same dataset. Read Interview your data from ProPublica data reporter and developer Derek Willis for more tips like these, even if you don’t quite yet know how to accomplish it all. Keyboard shortcuts For Mac users, it’s much easier to use Excel if you override the action of function keys while you’re in the program. In your Mac’s System Preferences, choose Keyboard, and select the box that says, “Use F1, F2, etc. as standard function keys.” (NOTE: If you have a MacBook Pro with a touch bar, this option isn’t there. Instead, go into the Shortcuts section of the keyboard options and turn off all of the options for Mission Control. Those are the ones that interfere with Excel.) Once you’ve done that, these keyboard shortcuts will work: To do this Windows or IMac Macbook Edit a cell F2 Ctl-U or F2 Toggle between absolute and relative references F4 Ctl-T or F4 Insert cut cells Ctl+ Ctl+ Delete a cell Ctl- Ctl- Select the top left of a spreadsheet Ctl-Home Ctl-Fn-Left arrow Move to the bottom right of a spreadsheet Ctl-End Ctl-Fn-Right arrow Select a region (a contiguous rectangle of cells that are filled out) Ctl -* Ctl-Shift_spacebar You should practice getting around a spreadsheet efficiently, since scrolling with the mouse while selecting is a lesson in frustration.",
    "url": "http://localhost:4000/cronkite-docs/excel/xlguides/xl-refresher.html",
    "relUrl": "/excel/xlguides/xl-refresher.html"
  },
  "40": {
    "id": "40",
    "title": "Data types / tidy data",
    "content": "Understanding data types Tidy data Tidy example Not so tidy Fixing untidy data, part 1 Understanding data types When you start working with structured data, there is an important unspoken rule: Each column should contain the same type of data, often broken into three categories: text, numbers and dates. This video goes through the basic types of data that you’ll find within a table such as an Excel worksheet, and what opportunities and traps await you. Most programming languages enforce a singe data type for each field or variable in a data frame or table. Instead of keeping the wrong version, it will turn all of them in to characters if it isn’t sure. It will turn all of the characters to missing data if there are only a few. As you get into programming, there are much more complicated data types that are really powerful. For example, a data type of “data frame” is used in R and Python to act like an Excel sheet. A json object can define a whole database in nested text. Those types will be covered in later modules. Tidy data “Like families, tidy datasets are all alike but every messy dataset is messy in its own way..” – Hadley Wickham, with apologies to Leo Tolstoy Hadley Wickham, who has done more to popularize the R programming language than almost anyone, wrote a seminal paper in 2014 called “Tidy Data”, which tackled the unspoken problem in data analysis: what does “clean” data mean, and what does “dirty” data mean? In it, he defined tidy data in this way: Every column is labeled and represents a variable, or something that differs from case to case. Examples include peoples’ names, counties, zip codes, height, age, income, or year. Every row, which may or may not be labeled, represents an observation, or an instance of each of the variables. An example might be your name, the county and zip code you live in, your height, age, income, and what year it is today. Each type of observational unit is in its own table, such as a page in an Excel file or a data frame in the R language. This suggests that every observation should be a the same unit of analysis. For example, there can be information about every student at your school in one table, and information about every department in your school in another. In other words, each column means one thing, each row is an instance of that thing. It means you’d rarely type two separate things into one cell, such as two names or two addresses. It also means you’ll often have a tall, narrow dataset – one with a few columns but many rows – rather than the short and wide datasets you often see. This makes it easy to sort, filter, and group your data into other information. It’s always easy to turn these tidy datasets into another form. It’s often difficult to turn messy datasets into something tidy. Look through Wickham’s list of ways that day is often untidy (about halfway down the document). Don’t worry about the programming in R that he suggests to fix the problems. Concentrate instead on the different ways that untidy data frequently appears. Tidy example Here’s an example, showing the beginning of a file obtained from New York State showing the names, locations and salaries of judges in the state’s Supreme Court (the main trial court in New York): It follows the simple rule that every column always means one and only one thing, and every row is an instance of that thing at the same level of analysis – the employee. It’s not perfect. The position titles don’t really show their level, but instead are sometimes used to show districts. In this sense, it’s not quite tidy: separating the title from the district would make it more useful. Not so tidy When you request data from the government, or seek out data on the Internet, you should look for tidy data. It is often the most pure form of the information available – it hasn’t been compiled into a report, or put together to make a nice printout. Instead, it is usually close to the way it’s been stored in the underlying database. Think of this when you see forms in the wild. They are often pretty tidy! A parking ticket is one instance of a thing, and each box filled out or left blank on the ticket is a characteristic of that thing. Sadly, the information and datasets you receive from others will often NOT adhere to tidy principles. Government “datasets” often have a lot of extra verbiage at the top and bottom, or are done in a hierarchical fashion, mixing detail and total at the same time. In this example, the Census Bureau seems to anticipate that you will print this data, not use it: If you just needed a few numbers from the release, this table is perfectly adequate. But if you wanted to combine it with other data or sort it in any way, it’s nearly useless. When you come across data that looks like this – with subtotals and different topics broken down the side, and many figures going across the top – look for other formats that might have been released, or talk with the agency to get something more useful. In this case, we’d look for the underlying data that created these tables. Here’s another example, this time from the Ohio Secretary of State, that lists the results of the 2016 election. Each page is one type of race – Congress, president, etc. – and each column shows the result for a candidate within a race. The rows are sometimes the precinct, and sometimes the total. There are totals mixed in with the precincts, and each of the candidates are listed left to right for every Congressional race. This makes it easy to read if you want to see who won. But each precinct would only have one race for U.S. Senate and one for the House, meaning that most of the values listed in this very, very wide spreadsheet are zero. Fixing untidy data, part 1 When you encounter untidy data like the example above, take out a piece of paper and sketch what a more tidy version of this spreadsheet might look like. What are the column headings, what does each row represent, and how might it be put together? It’s actually more complicated than it seems. Just remember that, so long as each table you create has a unique identifier that represents just one thing, you can always link them together. (Computer scientists shouldn’t confuse this with the “3rd normal form”. This is much less rigid about breaking out tables whenever there is repetition, but it does adhere to the idea of a single unit of analysis in each table.) This is an excellent exercise for you to do whenever you are working on data: envision success. If you had what you wanted, what might that look like? Here is an example spreadsheet of a series of ways that governments have provided data when it was requested. Go through them and think about how they might be useful – what might they look like? How could you use them to sort and filter? How could you add or count them effectively? Here’s a video that goes through what is wrong with them and how we fixed it.",
    "url": "http://localhost:4000/cronkite-docs/excel/xlguides/xl-tidydata.html",
    "relUrl": "/excel/xlguides/xl-tidydata.html"
  },
  "41": {
    "id": "41",
    "title": "Excel guides",
    "content": "",
    "url": "http://localhost:4000/cronkite-docs/excel/xlguides",
    "relUrl": "/excel/xlguides"
  },
  "42": {
    "id": "42",
    "title": "Excel practice",
    "content": "Here are some of the data files used in other tutorials: Phoenix city worker salaries Tempe opioid EMS calls Nebraska mountain lion sightings And some thoughts on where you can find more practice data: State and local open government web sites. Some are great, others, not so much. public.enigma.com and data.world, often cataloged in Google data",
    "url": "http://localhost:4000/cronkite-docs/excel/xlpractice",
    "relUrl": "/excel/xlpractice"
  },
  "43": {
    "id": "43",
    "title": "More on scraping",
    "content": "More on scraping more tools Outwit hub (now about $50, but powerful) import.io - said to be the easiest out there, but I’ve had trouble using it. By the time I get through trying to learn a tool, I might as well have learned to program it. Key parts of Python that will help with difficult sites: Scrapy - a whole library of scraping tools that has its own manual and syntax. I think this one uses XPath though, so it will be easier to incorporate that in. Selenium, for when you need to actually run a browser. * Building on the tutorial that is at the http://sarahcnyt.github.io/ire-toronto site These examples came up recently and I needed to memorialize it for myself. It’s just to help me remember. Start with Chrome inspection tool, right click on the element you want, then copy the XPath to give you a start. Finding Text Between Tags &lt;td nowrap=&quot;&quot;&gt; &lt;font face=&quot;arial&quot; size=&quot;2&quot;&gt; &lt;b&gt;ABRAMSON, JOEL ELIOT &amp;nbsp; &amp;nbsp; &lt;br&gt; &lt;/b&gt; JOEL E. ABRAMSON, P.C. &amp;nbsp; &amp;nbsp; &lt;br&gt; 271 MADISON AVE FL 22 &amp;nbsp; &amp;nbsp; &lt;br&gt; NEW YORK, NY 10016 &amp;nbsp; &amp;nbsp; &lt;br&gt; (212)599-7700 &amp;nbsp; &amp;nbsp; &lt;/font&gt; &lt;/td&gt; If the whole chunk is the 2nd column, it’s [2] How can I get the name, which is outlined by the &lt; b &gt; (bold) tag? That text is the first text() element below the “font” tag, Any of these will work: [2]/font/text() or [2]//text() or [2]/font/b/text() A little less intuitively: *[2]font[1]/child::text() or *[2]/node()/node() or *[2]/font/node()[position=1] The bold tag is the first child of the font tag. Technically, the “node” is everything between the font tags, then the second node is everything between the bold tags. Here’s how to get the city, state, zipcode from that chunk. (Use the last-1 because some firms had two-line addresses and others had only one. It would fail in cases where the city, state was the last row. *[2]/font/text()[position()=last()-1] or *[2]//text()[count(following-sibling::br)=1] What if you had a slightly different piece of code, this one a line from the Florida State football roster &lt;tr class=&quot;odd&quot;&gt; &lt;td class=&quot;number&quot;&gt;3&lt;/td&gt; &lt;td class=&quot;name&quot;&gt; &lt;span class=&quot;sortname&quot; style=&quot;display:none;&quot;&gt;Darby Ronald&lt;/span&gt; &lt;a title=&quot;Ronald Darby&quot; href=&quot;/ViewArticle.dbml?ATCLID=209573037&amp;amp;DB_OEM_ID=32900&quot;&gt;Ronald Darby&lt;/a&gt; &lt;/td&gt; &lt;td class=&quot;position&quot;&gt;CB&lt;/td&gt; &lt;td class=&quot;height&quot; nowrap=&quot;&quot;&gt;5-11&lt;/td&gt; &lt;td class=&quot;weight&quot;&gt;195&lt;/td&gt; &lt;td class=&quot;year&quot;&gt; &lt;span style=&quot;display:none;&quot;&gt;3&lt;/span&gt; Jr. &lt;/td&gt; &lt;td class=&quot;hometown &quot;&gt;Oxon Hill, Md. / Potomac&lt;/td&gt; &lt;/tr&gt; I’ll come back to this example later, so it’s worth having the whole snippet here. But all I care about right now is to pick up the “Jr” separate from the “3” in the class year. The first part is obvious, assuming we’re starting from the row. *[6]/span or *[@class=&quot;year&quot;]/span How about getting just the word, without the number? *[6]/node()[position()=3] or position()=last would work. It’s the 3rd node, not the second, because there is a span element in between. If it were text(), it would be the second. Recap: To get everything as text, without the tags in between, just use the position or name. To get the first text element, use text() or text()[position()=1]. It’s the same thing. To get the last text element, use text()[position()=last] To get a list of all of the elements, which has an entry for each tag and another for its text representation, use node() instead of text(). If for some reason you only want the list of tags without what they contain, use element() Finding elements Key XPath selectors are: descendant-or-self, which refers to everything below as well as yourself. It helps when you don’t know what level it might be. For example, in the html above, finding the “sortname” class wouldn’t depend on you knowing that it was two levels below. descendant-or-self::node()[@class=&quot;sortname&quot;] count(preceding-sibling::p) would count how many &lt; p &gt; tags came before. The opposite is following-sibling::tag. Leave off the the tag if you want just the number of elements, but they will be at the same level. You can use the count as a test, so that if you want the first two paragraphs, it can be: * text()[count(preceding-sibling::p)&lt;3] attributes of a tag, like onclick() action This was an interesting one, from the NYC ACRIS real estate document database. In this case, we were looking in an exercise for a company called Prevezon. There were about 30 records, just enough to make clicking into them a pain. If you get fewer than 100 records, you can scrape all using Chrome scraper. The key, though, is the link to the detail page. That is under the input button as an onclick event. Here is how I made the XPATH to get to that. substring(*[1]/div/font/input[1]/@onclick, 23,16)) This says, for each of the 1st tds, look for div/font/input[1] – he first input item (there are two). Then find the @click attribute, and take the 16 characters beginning at 23. To create the actual link, you can use concat(“root html”, your substring)",
    "url": "http://localhost:4000/cronkite-docs/assets/docs/xpath.html",
    "relUrl": "/assets/docs/xpath.html"
  }
  
}



          
        </div>
      </div>
    </div>
  </div>

</body>
</html>
